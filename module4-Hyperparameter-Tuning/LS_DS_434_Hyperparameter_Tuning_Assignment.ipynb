{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "# Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://drive.google.com/file/d/1dfbAsM9DwA7tYhInyflIpZnYs7VT-0AQ/view> \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNJ-tOBs4jM1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7043, 47)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "df = df.drop(columns = 'customerID')\n",
    "df.TotalCharges = df.TotalCharges.replace(' ', 0.0)\n",
    "df.TotalCharges = df.TotalCharges.astype(float)\n",
    "df.tenure = df.tenure.astype(float)\n",
    "df = pd.get_dummies(df)\n",
    "df.TotalCharges = df.TotalCharges.replace(' ', 0.0)\n",
    "df.TotalCharges = df.TotalCharges.astype(float)\n",
    "df.tenure = df.tenure.astype(float)\n",
    "df = pd.get_dummies(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Churn_Yes.values\n",
    "X = df.drop(columns = ['Churn_No','Churn_Yes']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline multi-level perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sandbox/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4695/4695 [==============================] - 4s 762us/step - loss: 8.5004 - acc: 0.4072\n",
      "Epoch 2/20\n",
      "4695/4695 [==============================] - 4s 952us/step - loss: 2.3790 - acc: 0.7625\n",
      "Epoch 3/20\n",
      "4695/4695 [==============================] - 5s 1ms/step - loss: 1.9544 - acc: 0.7681\n",
      "Epoch 4/20\n",
      "4695/4695 [==============================] - 3s 644us/step - loss: 1.5513 - acc: 0.7661\n",
      "Epoch 5/20\n",
      "4695/4695 [==============================] - 3s 592us/step - loss: 1.6666 - acc: 0.7544\n",
      "Epoch 6/20\n",
      "4695/4695 [==============================] - 3s 619us/step - loss: 1.5981 - acc: 0.7610\n",
      "Epoch 7/20\n",
      "4695/4695 [==============================] - 2s 374us/step - loss: 1.3938 - acc: 0.7602\n",
      "Epoch 8/20\n",
      "4695/4695 [==============================] - 2s 448us/step - loss: 1.3771 - acc: 0.7610\n",
      "Epoch 9/20\n",
      "4695/4695 [==============================] - 3s 703us/step - loss: 1.0235 - acc: 0.7646\n",
      "Epoch 10/20\n",
      "4695/4695 [==============================] - 4s 794us/step - loss: 1.2403 - acc: 0.7649 1s - loss: 1.248 - ETA: 0s - loss: 1.1201 -\n",
      "Epoch 11/20\n",
      "4695/4695 [==============================] - 4s 767us/step - loss: 1.0890 - acc: 0.7706\n",
      "Epoch 12/20\n",
      "4695/4695 [==============================] - 4s 782us/step - loss: 1.1245 - acc: 0.7614\n",
      "Epoch 13/20\n",
      "4695/4695 [==============================] - 4s 751us/step - loss: 1.0005 - acc: 0.7668\n",
      "Epoch 14/20\n",
      "4695/4695 [==============================] - 3s 651us/step - loss: 1.0235 - acc: 0.7663\n",
      "Epoch 15/20\n",
      "4695/4695 [==============================] - 5s 997us/step - loss: 0.7473 - acc: 0.7634\n",
      "Epoch 16/20\n",
      "4695/4695 [==============================] - 5s 1ms/step - loss: 0.9354 - acc: 0.7614\n",
      "Epoch 17/20\n",
      "4695/4695 [==============================] - 6s 1ms/step - loss: 0.7460 - acc: 0.7738A: 5s - loss: 0.5693 - acc: - ETA: 4s - loss: 0.587\n",
      "Epoch 18/20\n",
      "4695/4695 [==============================] - 4s 781us/step - loss: 0.9096 - acc: 0.7710\n",
      "Epoch 19/20\n",
      "4695/4695 [==============================] - 4s 859us/step - loss: 0.9091 - acc: 0.7700\n",
      "Epoch 20/20\n",
      "4695/4695 [==============================] - 3s 667us/step - loss: 0.8373 - acc: 0.7774\n",
      "2348/2348 [==============================] - 1s 351us/step\n",
      "4695/4695 [==============================] - 1s 208us/step\n",
      "Epoch 1/20\n",
      "4695/4695 [==============================] - 3s 720us/step - loss: 3.3054 - acc: 0.6554\n",
      "Epoch 2/20\n",
      "4695/4695 [==============================] - 2s 350us/step - loss: 1.1948 - acc: 0.7510\n",
      "Epoch 3/20\n",
      "4695/4695 [==============================] - 2s 378us/step - loss: 0.6566 - acc: 0.7689\n",
      "Epoch 4/20\n",
      "4695/4695 [==============================] - 2s 355us/step - loss: 0.7866 - acc: 0.7627 0s - loss: 0.7965 - acc: 0.762\n",
      "Epoch 5/20\n",
      "4695/4695 [==============================] - 2s 324us/step - loss: 0.6976 - acc: 0.7761\n",
      "Epoch 6/20\n",
      "4695/4695 [==============================] - 1s 315us/step - loss: 0.6811 - acc: 0.7757\n",
      "Epoch 7/20\n",
      "4695/4695 [==============================] - 2s 367us/step - loss: 0.6290 - acc: 0.7751 0s - loss: \n",
      "Epoch 8/20\n",
      "4695/4695 [==============================] - 1s 303us/step - loss: 0.8144 - acc: 0.7646 0s - loss: 0.7433 - acc:\n",
      "Epoch 9/20\n",
      "4695/4695 [==============================] - 2s 480us/step - loss: 0.9636 - acc: 0.7706\n",
      "Epoch 10/20\n",
      "4695/4695 [==============================] - 2s 373us/step - loss: 0.6881 - acc: 0.7774\n",
      "Epoch 11/20\n",
      "4695/4695 [==============================] - 2s 442us/step - loss: 0.8343 - acc: 0.7681\n",
      "Epoch 12/20\n",
      "4695/4695 [==============================] - 2s 527us/step - loss: 0.6195 - acc: 0.7793\n",
      "Epoch 13/20\n",
      "4695/4695 [==============================] - 3s 612us/step - loss: 0.7587 - acc: 0.7747\n",
      "Epoch 14/20\n",
      "4695/4695 [==============================] - 4s 821us/step - loss: 0.7482 - acc: 0.7715 1\n",
      "Epoch 15/20\n",
      "4695/4695 [==============================] - 4s 816us/step - loss: 0.6941 - acc: 0.7670\n",
      "Epoch 16/20\n",
      "4695/4695 [==============================] - 4s 868us/step - loss: 0.6164 - acc: 0.7781 0s - loss: 0.6111 - acc: 0.77\n",
      "Epoch 17/20\n",
      "4695/4695 [==============================] - 4s 943us/step - loss: 0.7407 - acc: 0.7700\n",
      "Epoch 18/20\n",
      "4695/4695 [==============================] - 4s 772us/step - loss: 0.5877 - acc: 0.7778\n",
      "Epoch 19/20\n",
      "4695/4695 [==============================] - 4s 792us/step - loss: 0.7813 - acc: 0.7747\n",
      "Epoch 20/20\n",
      "4695/4695 [==============================] - 4s 918us/step - loss: 0.6456 - acc: 0.7778\n",
      "2348/2348 [==============================] - 1s 566us/step\n",
      "4695/4695 [==============================] - 1s 279us/step\n",
      "Epoch 1/20\n",
      "4696/4696 [==============================] - 6s 1ms/step - loss: 3.0774 - acc: 0.6450\n",
      "Epoch 2/20\n",
      "4696/4696 [==============================] - 3s 555us/step - loss: 0.7986 - acc: 0.7668\n",
      "Epoch 3/20\n",
      "4696/4696 [==============================] - 3s 541us/step - loss: 0.6757 - acc: 0.7672\n",
      "Epoch 4/20\n",
      "4696/4696 [==============================] - 3s 541us/step - loss: 0.6141 - acc: 0.7756\n",
      "Epoch 5/20\n",
      "4696/4696 [==============================] - 3s 571us/step - loss: 0.6069 - acc: 0.7779\n",
      "Epoch 6/20\n",
      "4696/4696 [==============================] - 3s 566us/step - loss: 0.8175 - acc: 0.7583\n",
      "Epoch 7/20\n",
      "4696/4696 [==============================] - 3s 594us/step - loss: 0.5927 - acc: 0.7766\n",
      "Epoch 8/20\n",
      "4696/4696 [==============================] - 3s 714us/step - loss: 0.5604 - acc: 0.7775\n",
      "Epoch 9/20\n",
      "4696/4696 [==============================] - 4s 824us/step - loss: 0.5674 - acc: 0.7794\n",
      "Epoch 10/20\n",
      "4696/4696 [==============================] - 4s 842us/step - loss: 0.6490 - acc: 0.7702\n",
      "Epoch 11/20\n",
      "4696/4696 [==============================] - 4s 942us/step - loss: 0.6380 - acc: 0.7779\n",
      "Epoch 12/20\n",
      "4696/4696 [==============================] - 4s 921us/step - loss: 0.5939 - acc: 0.7741\n",
      "Epoch 13/20\n",
      "4696/4696 [==============================] - 4s 925us/step - loss: 0.5508 - acc: 0.7815 1s\n",
      "Epoch 14/20\n",
      "4696/4696 [==============================] - 5s 1ms/step - loss: 0.6720 - acc: 0.7724\n",
      "Epoch 15/20\n",
      "4696/4696 [==============================] - 5s 1ms/step - loss: 0.5689 - acc: 0.7824\n",
      "Epoch 16/20\n",
      "4696/4696 [==============================] - 7s 1ms/step - loss: 0.6686 - acc: 0.7802A: 4\n",
      "Epoch 17/20\n",
      "4696/4696 [==============================] - 7s 2ms/step - loss: 0.6205 - acc: 0.7721\n",
      "Epoch 18/20\n",
      "4696/4696 [==============================] - 6s 1ms/step - loss: 0.5331 - acc: 0.7815\n",
      "Epoch 19/20\n",
      "4696/4696 [==============================] - 5s 1ms/step - loss: 0.5549 - acc: 0.7836\n",
      "Epoch 20/20\n",
      "4696/4696 [==============================] - 4s 926us/step - loss: 0.5723 - acc: 0.7730\n",
      "2347/2347 [==============================] - 2s 659us/step\n",
      "4696/4696 [==============================] - 2s 416us/step\n",
      "Epoch 1/20\n",
      "7043/7043 [==============================] - 9s 1ms/step - loss: 3.1270 - acc: 0.6604\n",
      "Epoch 2/20\n",
      "7043/7043 [==============================] - 8s 1ms/step - loss: 1.1811 - acc: 0.7582A: 1s - loss: 1.1400 - a - ETA: 1s - loss:  - ETA: 0s - loss: 1.1815 - acc: 0.758\n",
      "Epoch 3/20\n",
      "7043/7043 [==============================] - 7s 1ms/step - loss: 1.0326 - acc: 0.7575\n",
      "Epoch 4/20\n",
      "7043/7043 [==============================] - 11s 2ms/step - loss: 1.2582 - acc: 0.7572\n",
      "Epoch 5/20\n",
      "7043/7043 [==============================] - 11s 2ms/step - loss: 0.8218 - acc: 0.7617: 1s - loss:\n",
      "Epoch 6/20\n",
      "7043/7043 [==============================] - 7s 930us/step - loss: 0.6080 - acc: 0.7784\n",
      "Epoch 7/20\n",
      "7043/7043 [==============================] - 6s 884us/step - loss: 0.8919 - acc: 0.7626\n",
      "Epoch 8/20\n",
      "7043/7043 [==============================] - 8s 1ms/step - loss: 0.6820 - acc: 0.7748\n",
      "Epoch 9/20\n",
      "7043/7043 [==============================] - 9s 1ms/step - loss: 0.8308 - acc: 0.7696\n",
      "Epoch 10/20\n",
      "7043/7043 [==============================] - 6s 891us/step - loss: 0.8747 - acc: 0.7701\n",
      "Epoch 11/20\n",
      "7043/7043 [==============================] - 7s 934us/step - loss: 0.5918 - acc: 0.7769\n",
      "Epoch 12/20\n",
      "7043/7043 [==============================] - 8s 1ms/step - loss: 0.8160 - acc: 0.7738\n",
      "Epoch 13/20\n",
      "7043/7043 [==============================] - 10s 1ms/step - loss: 0.8586 - acc: 0.7636: 1s - loss: 0.\n",
      "Epoch 14/20\n",
      "7043/7043 [==============================] - 9s 1ms/step - loss: 1.0063 - acc: 0.7612\n",
      "Epoch 15/20\n",
      "7043/7043 [==============================] - 8s 1ms/step - loss: 0.6681 - acc: 0.7786\n",
      "Epoch 16/20\n",
      "7043/7043 [==============================] - 6s 913us/step - loss: 0.8310 - acc: 0.7734\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043/7043 [==============================] - 9s 1ms/step - loss: 0.8604 - acc: 0.7718\n",
      "Epoch 18/20\n",
      "7043/7043 [==============================] - 5s 739us/step - loss: 0.6181 - acc: 0.7775\n",
      "Epoch 19/20\n",
      "7043/7043 [==============================] - 6s 805us/step - loss: 0.7961 - acc: 0.7687\n",
      "Epoch 20/20\n",
      "7043/7043 [==============================] - 7s 969us/step - loss: 0.6452 - acc: 0.7730\n",
      "Best: 0.7983813721162427 using {'batch_size': 20, 'epochs': 20}\n",
      "\n",
      "Means: 0.7983813721162427, Stdev: 0.006833770971778664 with: {'batch_size': 20, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=45, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [20]\n",
    "epochs = [20]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "print()\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.798 using {'batch_size': 20, 'epochs': 20}\n",
      "\n",
      "Means: 0.798, Stdev: 0.007 with: {'batch_size': 20, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")\n",
    "print()\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean:.3f}, Stdev: {stdev:.3f} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
