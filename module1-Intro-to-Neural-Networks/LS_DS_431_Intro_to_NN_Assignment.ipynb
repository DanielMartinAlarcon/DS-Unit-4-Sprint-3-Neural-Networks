{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "The first layer of a NN. It's a series of variables that correspond directly to a feature in the data. In image recognition, for example, they could be the brightness of a particular pixel.  \n",
    "\n",
    "### Hidden Layer:\n",
    "Internal layers of the NN, connecting input and output layers.  They are a series of functions, each of which takes in the values of all the variables in the previous layer (starting with the input layer) and produces a single number as output.  The output value is generated as a sum of the values of all the nodes in the previous layer, each multiplied by a weight.  That summation gets passed through a squishification function, and added to a bias to produce the output.  Hidden layers don't need to correspond to any recognizable feature of the outside world.\n",
    "\n",
    "### Output Layer:\n",
    "The final layer of the NN.  Each node is a function like the hidden layers, but its output corresponds to the NN's predictions for a single outcome variable.  Note that the nodes in this layer don't usually have an activation function.\n",
    "\n",
    "### Neuron:\n",
    "Each of the nodes in the NN.  They have a structure analogous to that of biological neurons. Neurons read the activation state of a bunch of neurons in the previous layer (each weighed differently), and use that information to produce a single output value. \n",
    "\n",
    "### Weight:\n",
    "Each neuron in one layer is connected to all the neurons in the next layer. The strength of each connection (called the weight) determines how sensitive the value of neuron N+1 is to the value of neuron N.\n",
    "\n",
    "### Activation Function:\n",
    "Each neuron must aggregate inputs and produce a single output.  The activation function shapes that output to be within useful bounds.  One can use several possible activation functions, but common functions (sigmoid, tanh) will map the whole numberline to a small range ((0,1) or (-1,1)) or get rid of negative numbers (ReLU).  Note that all the nodes in a layer of the NN tend to have the same activation function.\n",
    "\n",
    "### Node Map:\n",
    "A graphical representation of a NN that shows its layers, the node types within them, and how all the nodes connect to each other.\n",
    "\n",
    "### Perceptron:\n",
    "The simplest feedforward neural network.  It is an artificial neuron with a single layer and a unit step function as its activation function.  It connects a series of input nodes with a single output node.  It is also known as a linear binary classifier, and it is usually used to classify data into two parts.\n",
    "\n",
    "### Bias\n",
    "A constant term that gets included in a node's weighted sum of the activations of the nodes in the previous layer.  Bias tells us the baseline level of activation of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The activation of the nodes in the input layer is proportional to the data itself. Each feature in the dataset corresponds to a particular node in the activation layer. Starting after the input, all the neurons in the Nth layer are connected to all the neurons in the N+1th layer.  Each connection (between one node and the next) has an associated weight, which determines how important that connection is.  Higher weights mean that the activation of the N+1th neuron will respond more strongly to activation in the Nth. Each neuron in the hidden layer also has a bias, which gets added in the summation of activations from its predecessor neurons. That way each neuron has a baseline activity regardless of the activation of its predecessors. Each node sums up its inputs and passes them through an activation function, which can map the sum of outputs to a particular range (eg (0-1)) or otherwise transform it in a useful way. \n",
    "\n",
    "The overall flow of information looks like this.  An input neuron activates based on the value of a feature in the dataset.  The activity of that neuron influences all the neurons in the next layer, proportionally to the weights connecting them to the first. Each neuron in the 2nd layer decides on its own output value by considering the contributions from everything in the first layer plus a bias term, all passed through the activation function.  The process then repeats for the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70795059],\n",
       "       [-0.01152633],\n",
       "       [ 0.69312297]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.random.seed(2)\n",
    "\n",
    "inputs = np.array([[0,0,1],\n",
    "                   [1,0,1],\n",
    "                   [0,1,1],\n",
    "                   [1,1,1]])\n",
    "\n",
    "correct_outputs = [[1],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [0]]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Initialize weights, all in the range (-1, 1)\n",
    "weights = np.random.random((3,1))*2 - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[-12.0509239]\n",
      " [-12.0509239]\n",
      " [ 18.1251759]]\n",
      "Output After Training:\n",
      "[[0.99999999]\n",
      " [0.99770371]\n",
      " [0.99770371]\n",
      " [0.00253106]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "\n",
    "    # Weighted sum of inputs and weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "    # Activate with sigmoid function\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "    # Calculate Error\n",
    "    error = correct_outputs - activated_output\n",
    "\n",
    "    # Calculate weight adjustments with sigmoid_derivative\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "\n",
    "print('optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(activated_output)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
