{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "The first layer of a NN. It's a series of variables that correspond directly to a feature in the data. In image recognition, for example, they could be the brightness of a particular pixel.  \n",
    "\n",
    "### Hidden Layer:\n",
    "Internal layers of the NN, connecting input and output layers.  They are a series of functions, each of which takes in the values of all the variables in the previous layer (starting with the input layer) and produces a single number as output.  The output value is generated as a sum of the values of all the nodes in the previous layer, each multiplied by a weight.  That summation gets passed through a squishification function, and added to a bias to produce the output.  Hidden layers don't need to correspond to any recognizable feature of the outside world.\n",
    "\n",
    "### Output Layer:\n",
    "The final layer of the NN.  Each node is a function like the hidden layers, but its output corresponds to the NN's predictions for a single outcome variable.  Note that the nodes in this layer don't usually have an activation function.\n",
    "\n",
    "### Neuron:\n",
    "Each of the nodes in the NN.  They have a structure analogous to that of biological neurons. Neurons read the activation state of a bunch of neurons in the previous layer (each weighed differently), and use that information to produce a single output value. \n",
    "\n",
    "### Weight:\n",
    "Each neuron in one layer is connected to all the neurons in the next layer. The strength of each connection (called the weight) determines how sensitive the value of neuron N+1 is to the value of neuron N.\n",
    "\n",
    "### Activation Function:\n",
    "Each neuron must aggregate inputs and produce a single output.  The activation function shapes that output to be within useful bounds.  One can use several possible activation functions, but common functions (sigmoid, tanh) will map the whole numberline to a small range ((0,1) or (-1,1)) or get rid of negative numbers (ReLU).  Note that all the nodes in a layer of the NN tend to have the same activation function.\n",
    "\n",
    "### Node Map:\n",
    "A graphical representation of a NN that shows its layers, the node types within them, and how all the nodes connect to each other.\n",
    "\n",
    "### Perceptron:\n",
    "The simplest feedforward neural network.  It is an artificial neuron with a single layer and a unit step function as its activation function.  It connects a series of input nodes with a single output node.  It is also known as a linear binary classifier, and it is usually used to classify data into two parts.\n",
    "\n",
    "### Bias\n",
    "A constant term that gets included in a node's weighted sum of the activations of the nodes in the previous layer.  Bias tells us the baseline level of activation of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The activation of the nodes in the input layer is proportional to the data itself. Each feature in the dataset corresponds to a particular node in the activation layer. Starting after the input, all the neurons in the Nth layer are connected to all the neurons in the N+1th layer.  Each connection (between one node and the next) has an associated weight, which determines how important that connection is.  Higher weights mean that the activation of the N+1th neuron will respond more strongly to activation in the Nth. Each neuron in the hidden layer also has a bias, which gets added in the summation of activations from its predecessor neurons. That way each neuron has a baseline activity regardless of the activation of its predecessors. Each node sums up its inputs and passes them through an activation function, which can map the sum of outputs to a particular range (eg (0-1)) or otherwise transform it in a useful way. \n",
    "\n",
    "The overall flow of information looks like this.  An input neuron activates based on the value of a feature in the dataset.  The activity of that neuron influences all the neurons in the next layer, proportionally to the weights connecting them to the first. Each neuron in the 2nd layer decides on its own output value by considering the contributions from everything in the first layer plus a bias term, all passed through the activation function.  The process then repeats for the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.84070905],\n",
       "       [ 0.01049218],\n",
       "       [-0.86942699]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.random.seed(2)\n",
    "\n",
    "inputs = np.array([[0,0,1],\n",
    "                   [1,0,1],\n",
    "                   [0,1,1],\n",
    "                   [1,1,1]])\n",
    "\n",
    "correct_outputs = [[1],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [0]]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Initialize weights, all in the range (-1, 1)\n",
    "weights = np.random.random((3,1))*2 - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[-11.83931027]\n",
      " [-11.83931027]\n",
      " [ 17.80783017]]\n",
      "Output After Training:\n",
      "[[0.99999998]\n",
      " [0.99744825]\n",
      " [0.99744825]\n",
      " [0.00281299]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "\n",
    "    # Weighted sum of inputs and weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "    # Activate with sigmoid function\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "    # Calculate Error\n",
    "    error = correct_outputs - activated_output\n",
    "\n",
    "    # Calculate weight adjustments with sigmoid_derivative\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "\n",
    "print('optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(activated_output)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "# Copied from https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3\n",
    "# This class uses ReLU activation function and adjusts the weights by \n",
    "# multiplying times the error (label-prediction) and the learning rate. \n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0            \n",
    "        return activation\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.threshold):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll test it on the same NAND gate defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],\n",
    "                   [1,0],\n",
    "                   [0,1],\n",
    "                   [1,1]])\n",
    "\n",
    "correct_outputs = np.array([[1],\n",
    "                           [1],\n",
    "                           [1],\n",
    "                           [0]])\n",
    "\n",
    "pn = Perceptron(no_of_inputs=2, threshold=100, learning_rate=0.01)\n",
    "pn.train(inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03, -0.01, -0.02])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's predictions for NAND gate\n",
      "0 0 -> 1\n",
      "1 0 -> 1\n",
      "0 1 -> 1\n",
      "1 1 -> 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron's predictions for NAND gate\")\n",
    "for row in inputs:\n",
    "    print(f'{row[0]} {row[1]} -> {pn.predict(row)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for something more complicated, let's try the Pima Indians Diatabetes database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Outcome').values\n",
    "y = df.Outcome.values\n",
    "no_of_inputs = pima_inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-2.9      8.97     1.14    -2.85    -1.9      1.63     0.635    0.56346\n",
      " -1.2    ]\n",
      "Accuracy: 0.5885416666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 10 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=10, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-28.28     15.3       0.97     -3.48     -2.74      1.59     -0.127\n",
      "   6.89463  -2.25   ]\n",
      "Accuracy: 0.6536458333333334\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 100 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=100, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-237.44      13.58       2.61      -3.4       -2.16       2.52\n",
      "    2.666     46.19301   -1.95   ]\n",
      "Accuracy: 0.6171875\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 1000 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=1000, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-847.85      12.63       5.72      -3.94      -1.88       1.59\n",
      "    8.516    124.50169    1.21   ]\n",
      "Accuracy: 0.6822916666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 10000 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=10000, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
