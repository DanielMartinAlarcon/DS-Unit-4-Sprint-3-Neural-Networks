{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "### Neuron:\n",
    "Each of the nodes in the NN, with a structure analogous to that of brain cells. Neurons are characterized by their connections to a bunch of inputs. These inputs can be features in the data, outputs from neurons in an earlier layer, or a bias term that can be updated when the network learns.  The connections (sometimes called synapses) each have their own characteristic weight.  All the inputs get linearly combined; that is, summed up proportional to the connection weight.  Once combined, these inputs get passed through an activation function that transforms them non-linearly into the neuron's output value. This single output value then gets passed to the next stage, which can be the output of the network or another layer of neurons.\n",
    "\n",
    "### Input Layer:\n",
    "The first layer of a NN.  Like all layers, it is made up of neurons.  Unlike other layers, the neurons here receive inputs directly from the dataset, usually one neuron per feature with no biases. In image recognition problems, for example, each neuron in the input layer would be associated with the brightness of a particular pixel in the image.  \n",
    "\n",
    "### Hidden Layer:\n",
    "Internal layers of the NN, connecting input and output layers. The neurons here are connected to neurons in the previous layer; in the case of dense layers, every neuron in the previous layer is connected to every neuron in this one. Hidden layers don't need to correspond to any recognizable feature of the outside world.\n",
    "\n",
    "### Output Layer:\n",
    "The final layer of the NN.  It contains one neuron for every output variable in the network. The activation functions here should correspond to the desired form of the output.  A sigmoid activation function, for example, is great when the output should represent the probability of something (because sigmoid maps to output values between 0 and 1).\n",
    "\n",
    "### Activation Function:\n",
    "Each neuron must aggregate inputs and produce a single output.  The activation function shapes that output to be within useful bounds, and adds a non-linearity that enables the network to model complex non-linear behaviors.  One can use several possible activation functions, but common functions (sigmoid, tanh) will map the whole numberline to a small range ((0,1) or (-1,1)) or get rid of negative numbers (ReLU).  Note that all the nodes in a layer of the NN tend to have the same activation function.\n",
    "\n",
    "### Backpropagation\n",
    "A neural network learns by updating all of its weights and biases, based on how its output is different from what its output should be.  The process that achieves this update is called backpropagation.  You start with the predictions, comparing each of them to the known values of the correct answer and using a cost function to determine (with a single number) how far the two answers are from each other. Calculating the gradient of the loss function tells us which way each of the predictions would have to change in order to minimize error, and which changes would have the greatest effect on the error. \n",
    "\n",
    "The weights immediately before the output layer are updated based on the gradient of the loss function at each output neuron, proportional to their importance.  The gradient and the weights of the connections allows us to establish the blame of each input for the error in the current layer. The error in each of the N-1 neurons can determined by summing up all the blame from each of its connections, allowing us to repeat the process for the N-1 layer.  \n",
    "\n",
    "Backpropagation thus moves back through the network, updating the weights of all the connections (and the values of all the biases) so as to decrease the overall error as determined by the loss function. Each update is a small step, proportional to the gradient but bounded by the learning rate, so that the network slowly moves towards the minimum loss and the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, epochs=100, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "    \n",
    "    # This perceptron has one neuron for every input, plus a bias.\n",
    "    # Its activation function is a simple step function\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0            \n",
    "        return activation\n",
    "    \n",
    "    # It prints out its current predictions at every training epoch,\n",
    "    # and updates all the weights and biases with each step\n",
    "    def train(self, training_inputs, labels):\n",
    "        print('   Desired      Actual             Weights')\n",
    "        for _ in range(self.epochs):\n",
    "            all_preds = []\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "                all_preds.append(prediction)\n",
    "            print([int(x) for x in labels], all_preds, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Desired      Actual             Weights\n",
      "[1, 0, 0, 0] [0, 1, 1, 0] [-0.01  0.    0.   -0.01]\n",
      "[1, 0, 0, 0] [0, 1, 0, 0] [-0.01  0.    0.01 -0.01]\n",
      "[1, 0, 0, 0] [0, 1, 0, 0] [-0.01  0.    0.02 -0.01]\n",
      "[1, 0, 0, 0] [0, 1, 1, 0] [-0.02  0.    0.02 -0.02]\n",
      "[1, 0, 0, 0] [0, 0, 1, 0] [-0.02  0.01  0.02 -0.02]\n",
      "[1, 0, 0, 0] [0, 0, 1, 0] [-0.02  0.02  0.02 -0.02]\n",
      "[1, 0, 0, 0] [0, 1, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [-0.02  0.02  0.03 -0.02]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,1,1],\n",
    "              [1,0,1],\n",
    "              [0,1,1],\n",
    "              [0,0,1]])\n",
    "\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "pn = Perceptron(no_of_inputs=3, epochs=15, learning_rate=0.01)\n",
    "pn.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's predictions for AND gate\n",
      "1 1 1-> 1\n",
      "1 0 1-> 0\n",
      "0 1 1-> 0\n",
      "0 0 1-> 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron's predictions for AND gate\")\n",
    "for row in x:\n",
    "    print(f'{row[0]} {row[1]} {row[2]}-> {pn.predict(row)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This perceptron converges towards the right answer in less than 10 training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is clean of nulls and contains just numeric features, so it can be used \n",
    "# without much cleaning.\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "scaler = RobustScaler()\n",
    "\n",
    "y = df.target.values\n",
    "# Normalize all the columns\n",
    "x = scaler.fit_transform(df.drop(columns='target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 13)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like we'll have 13 input variables\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sandbox/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline\n",
    "I'll use one neuron for each input variable, two hidden layers with 13 neurons each and ReLU activation, and sigmoid activation for the one output variable.  The best loss function for binary classification is binary crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7854 - acc: 0.4587\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 177us/step - loss: 0.7492 - acc: 0.4545\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.7221 - acc: 0.4587\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 181us/step - loss: 0.7015 - acc: 0.5041\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 189us/step - loss: 0.6842 - acc: 0.5372\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 242us/step - loss: 0.6700 - acc: 0.6074\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.6572 - acc: 0.6405\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 158us/step - loss: 0.6448 - acc: 0.6694\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 158us/step - loss: 0.6315 - acc: 0.6901\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 156us/step - loss: 0.6173 - acc: 0.7149\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 0.6028 - acc: 0.7397\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 0.5887 - acc: 0.7562\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.5736 - acc: 0.7686\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 156us/step - loss: 0.5583 - acc: 0.7727\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.5427 - acc: 0.7810\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 0.5276 - acc: 0.7934\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.5131 - acc: 0.7975\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 141us/step - loss: 0.4997 - acc: 0.7893\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 0.4853 - acc: 0.8099\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 154us/step - loss: 0.4717 - acc: 0.8099\n",
      "61/61 [==============================] - 0s 3ms/step\n",
      "242/242 [==============================] - 0s 83us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7201 - acc: 0.4421\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 122us/step - loss: 0.6965 - acc: 0.4959\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 147us/step - loss: 0.6782 - acc: 0.5124\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 154us/step - loss: 0.6637 - acc: 0.5331\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 104us/step - loss: 0.6522 - acc: 0.5785\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 103us/step - loss: 0.6414 - acc: 0.6240\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 135us/step - loss: 0.6329 - acc: 0.6364\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 0.6236 - acc: 0.6488\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 102us/step - loss: 0.6145 - acc: 0.6446\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 0.6045 - acc: 0.6736\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 140us/step - loss: 0.5954 - acc: 0.7025\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 144us/step - loss: 0.5851 - acc: 0.7107\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 0.5746 - acc: 0.7107\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 127us/step - loss: 0.5622 - acc: 0.7149\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 121us/step - loss: 0.5490 - acc: 0.7231\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 0.5337 - acc: 0.7355\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 117us/step - loss: 0.5209 - acc: 0.7521\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 125us/step - loss: 0.5064 - acc: 0.7479\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 129us/step - loss: 0.4930 - acc: 0.7562\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 134us/step - loss: 0.4799 - acc: 0.7645\n",
      "61/61 [==============================] - 0s 3ms/step\n",
      "242/242 [==============================] - 0s 44us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6540 - acc: 0.6570\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 0.6345 - acc: 0.7107\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.6153 - acc: 0.7314\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 158us/step - loss: 0.5965 - acc: 0.7479\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.5761 - acc: 0.7727\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.5552 - acc: 0.7851\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 148us/step - loss: 0.5350 - acc: 0.8017\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 122us/step - loss: 0.5130 - acc: 0.7975\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 0.4926 - acc: 0.7934\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 130us/step - loss: 0.4728 - acc: 0.8058\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 134us/step - loss: 0.4546 - acc: 0.8058\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 121us/step - loss: 0.4397 - acc: 0.8099\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 122us/step - loss: 0.4271 - acc: 0.8182\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 115us/step - loss: 0.4157 - acc: 0.8264\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 119us/step - loss: 0.4051 - acc: 0.8306\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 111us/step - loss: 0.3958 - acc: 0.8347\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 0.3868 - acc: 0.8388\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 117us/step - loss: 0.3812 - acc: 0.8347\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 108us/step - loss: 0.3751 - acc: 0.8430\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 107us/step - loss: 0.3708 - acc: 0.8512\n",
      "61/61 [==============================] - 0s 3ms/step\n",
      "242/242 [==============================] - 0s 52us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.6636 - acc: 0.5638\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 111us/step - loss: 0.6293 - acc: 0.6543\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 149us/step - loss: 0.6009 - acc: 0.7037\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 131us/step - loss: 0.5766 - acc: 0.7284\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 124us/step - loss: 0.5549 - acc: 0.7613\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 112us/step - loss: 0.5352 - acc: 0.7819\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 125us/step - loss: 0.5153 - acc: 0.7778\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 129us/step - loss: 0.4966 - acc: 0.7778\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 115us/step - loss: 0.4809 - acc: 0.7737\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 128us/step - loss: 0.4656 - acc: 0.7860\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 122us/step - loss: 0.4520 - acc: 0.7984\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 120us/step - loss: 0.4391 - acc: 0.8148\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 138us/step - loss: 0.4256 - acc: 0.8230\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 127us/step - loss: 0.4152 - acc: 0.8230\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 120us/step - loss: 0.4067 - acc: 0.8313\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 124us/step - loss: 0.3969 - acc: 0.8189\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 124us/step - loss: 0.3897 - acc: 0.8189\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 127us/step - loss: 0.3823 - acc: 0.8313\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 125us/step - loss: 0.3759 - acc: 0.8230\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 116us/step - loss: 0.3701 - acc: 0.8395\n",
      "60/60 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 44us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 0.8091 - acc: 0.4650\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 159us/step - loss: 0.7567 - acc: 0.4938\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 0.7205 - acc: 0.5556\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 142us/step - loss: 0.6895 - acc: 0.5926\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 125us/step - loss: 0.6653 - acc: 0.6379\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 133us/step - loss: 0.6441 - acc: 0.6790\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 0.6250 - acc: 0.6914\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 147us/step - loss: 0.6050 - acc: 0.7243\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 0.5851 - acc: 0.7407\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 145us/step - loss: 0.5644 - acc: 0.7737\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 123us/step - loss: 0.5448 - acc: 0.7737\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 148us/step - loss: 0.5268 - acc: 0.7984\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 116us/step - loss: 0.5073 - acc: 0.8066\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 116us/step - loss: 0.4882 - acc: 0.8148\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 124us/step - loss: 0.4697 - acc: 0.8025\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 116us/step - loss: 0.4514 - acc: 0.8148\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 111us/step - loss: 0.4344 - acc: 0.8230\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 123us/step - loss: 0.4168 - acc: 0.8395\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 146us/step - loss: 0.3990 - acc: 0.8477\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 129us/step - loss: 0.3827 - acc: 0.8477\n",
      "60/60 [==============================] - 0s 4ms/step\n",
      "243/243 [==============================] - 0s 57us/step\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6851 - acc: 0.5347\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 115us/step - loss: 0.6645 - acc: 0.5677\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 132us/step - loss: 0.6458 - acc: 0.6139\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 133us/step - loss: 0.6273 - acc: 0.6469\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 143us/step - loss: 0.6085 - acc: 0.6964\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 130us/step - loss: 0.5902 - acc: 0.7393\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 118us/step - loss: 0.5697 - acc: 0.7756\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 142us/step - loss: 0.5486 - acc: 0.7888\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 136us/step - loss: 0.5266 - acc: 0.8053\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 131us/step - loss: 0.5044 - acc: 0.8119\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 111us/step - loss: 0.4818 - acc: 0.8218\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 131us/step - loss: 0.4609 - acc: 0.8284\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 123us/step - loss: 0.4418 - acc: 0.8383\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 140us/step - loss: 0.4243 - acc: 0.8482\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 122us/step - loss: 0.4058 - acc: 0.8416\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 128us/step - loss: 0.3921 - acc: 0.8482\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 125us/step - loss: 0.3794 - acc: 0.8548\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 113us/step - loss: 0.3692 - acc: 0.8548\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 127us/step - loss: 0.3593 - acc: 0.8548\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 124us/step - loss: 0.3512 - acc: 0.8515\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=20, \n",
    "                        epochs=20,verbose=1)\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = dict()\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                    n_jobs=1, cv=kfold)\n",
    "\n",
    "# Fit\n",
    "grid_result1 = grid.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 81.188% using {}\n",
      "\n",
      "Accuracy: 81.188%\n",
      "Stdev   : 2.954%\n",
      "Params  : {}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid_result1\n",
    "\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_*100:.3f}% using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'''\n",
    "Accuracy: {mean*100:.3f}%\n",
    "Stdev   : {stdev*100:.3f}%\n",
    "Params  : {param}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tune batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7751 - acc: 0.4587\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 267us/step - loss: 0.7262 - acc: 0.4752\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 255us/step - loss: 0.6942 - acc: 0.5124\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 268us/step - loss: 0.6702 - acc: 0.5909\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 274us/step - loss: 0.6473 - acc: 0.6612\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 309us/step - loss: 0.6249 - acc: 0.6983\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 258us/step - loss: 0.6006 - acc: 0.7397\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 268us/step - loss: 0.5747 - acc: 0.7603\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 256us/step - loss: 0.5483 - acc: 0.7810\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 264us/step - loss: 0.5222 - acc: 0.7934\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 244us/step - loss: 0.4972 - acc: 0.8017\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 245us/step - loss: 0.4743 - acc: 0.8099\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 250us/step - loss: 0.4522 - acc: 0.8099\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 224us/step - loss: 0.4321 - acc: 0.8182\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 247us/step - loss: 0.4131 - acc: 0.8223\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 267us/step - loss: 0.3961 - acc: 0.8223\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 334us/step - loss: 0.3796 - acc: 0.8264\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 366us/step - loss: 0.3663 - acc: 0.8347\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 271us/step - loss: 0.3516 - acc: 0.8430\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 278us/step - loss: 0.3407 - acc: 0.8471\n",
      "61/61 [==============================] - 0s 6ms/step\n",
      "242/242 [==============================] - 0s 165us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7133 - acc: 0.4504\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 269us/step - loss: 0.6800 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 276us/step - loss: 0.6563 - acc: 0.5661\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 291us/step - loss: 0.6376 - acc: 0.6157\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 359us/step - loss: 0.6212 - acc: 0.6570\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 247us/step - loss: 0.6048 - acc: 0.6736\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 250us/step - loss: 0.5900 - acc: 0.6983\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 270us/step - loss: 0.5707 - acc: 0.7066\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 261us/step - loss: 0.5493 - acc: 0.7231\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 263us/step - loss: 0.5261 - acc: 0.7397\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 223us/step - loss: 0.5028 - acc: 0.7479\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 226us/step - loss: 0.4807 - acc: 0.7603\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 219us/step - loss: 0.4626 - acc: 0.7727\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 368us/step - loss: 0.4453 - acc: 0.7810\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 272us/step - loss: 0.4310 - acc: 0.7810\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 347us/step - loss: 0.4186 - acc: 0.7934\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 303us/step - loss: 0.4104 - acc: 0.8017\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 296us/step - loss: 0.4014 - acc: 0.8099\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 302us/step - loss: 0.3936 - acc: 0.8182\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 297us/step - loss: 0.3869 - acc: 0.8182\n",
      "61/61 [==============================] - 0s 8ms/step\n",
      "242/242 [==============================] - 0s 161us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6499 - acc: 0.6653\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 285us/step - loss: 0.6195 - acc: 0.7231\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 442us/step - loss: 0.5860 - acc: 0.7686\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 352us/step - loss: 0.5512 - acc: 0.7975\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 323us/step - loss: 0.5143 - acc: 0.7975\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 287us/step - loss: 0.4802 - acc: 0.8099\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 290us/step - loss: 0.4526 - acc: 0.8140\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 328us/step - loss: 0.4277 - acc: 0.8182\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 346us/step - loss: 0.4098 - acc: 0.8306\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 322us/step - loss: 0.3951 - acc: 0.8347\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 487us/step - loss: 0.3830 - acc: 0.8430\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.3745 - acc: 0.8471\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 315us/step - loss: 0.3662 - acc: 0.8430\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 330us/step - loss: 0.3595 - acc: 0.8430\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 299us/step - loss: 0.3543 - acc: 0.8430\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 267us/step - loss: 0.3491 - acc: 0.8430\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 303us/step - loss: 0.3450 - acc: 0.8388\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 379us/step - loss: 0.3426 - acc: 0.8388\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 373us/step - loss: 0.3401 - acc: 0.8430\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 330us/step - loss: 0.3372 - acc: 0.8512\n",
      "61/61 [==============================] - 0s 7ms/step\n",
      "242/242 [==============================] - 0s 186us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.6528 - acc: 0.6008\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 245us/step - loss: 0.6031 - acc: 0.7078\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 284us/step - loss: 0.5635 - acc: 0.7654\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 277us/step - loss: 0.5302 - acc: 0.7778\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 269us/step - loss: 0.5009 - acc: 0.7778\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 269us/step - loss: 0.4756 - acc: 0.7942\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 282us/step - loss: 0.4523 - acc: 0.8230\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 268us/step - loss: 0.4313 - acc: 0.8230\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 266us/step - loss: 0.4157 - acc: 0.8189\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 273us/step - loss: 0.4006 - acc: 0.8354\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 272us/step - loss: 0.3883 - acc: 0.8313\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 263us/step - loss: 0.3780 - acc: 0.8354\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 307us/step - loss: 0.3685 - acc: 0.8395\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 308us/step - loss: 0.3602 - acc: 0.8436\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 286us/step - loss: 0.3537 - acc: 0.8519\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 293us/step - loss: 0.3463 - acc: 0.8519\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 276us/step - loss: 0.3403 - acc: 0.8601\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 265us/step - loss: 0.3357 - acc: 0.8683\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 262us/step - loss: 0.3308 - acc: 0.8724\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 293us/step - loss: 0.3262 - acc: 0.8724\n",
      "60/60 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 127us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.7917 - acc: 0.4733\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 257us/step - loss: 0.7195 - acc: 0.5514\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 230us/step - loss: 0.6720 - acc: 0.6296\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 259us/step - loss: 0.6339 - acc: 0.6872\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 253us/step - loss: 0.6009 - acc: 0.7284\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 236us/step - loss: 0.5680 - acc: 0.7613\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 270us/step - loss: 0.5364 - acc: 0.7901\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 293us/step - loss: 0.5051 - acc: 0.8107\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 271us/step - loss: 0.4729 - acc: 0.8148\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 257us/step - loss: 0.4409 - acc: 0.8313\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 315us/step - loss: 0.4118 - acc: 0.8395\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 298us/step - loss: 0.3874 - acc: 0.8436\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 253us/step - loss: 0.3659 - acc: 0.8642\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 272us/step - loss: 0.3524 - acc: 0.8724\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 250us/step - loss: 0.3406 - acc: 0.8724\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 257us/step - loss: 0.3305 - acc: 0.8724\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 248us/step - loss: 0.3241 - acc: 0.8601\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 254us/step - loss: 0.3180 - acc: 0.8683\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 263us/step - loss: 0.3133 - acc: 0.8683\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 310us/step - loss: 0.3080 - acc: 0.8765\n",
      "60/60 [==============================] - 0s 7ms/step\n",
      "243/243 [==============================] - 0s 138us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6860 - acc: 0.5496\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.6690 - acc: 0.5702\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 155us/step - loss: 0.6546 - acc: 0.5909\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 0.6393 - acc: 0.6322\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 160us/step - loss: 0.6248 - acc: 0.6777\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 164us/step - loss: 0.6103 - acc: 0.6942\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 148us/step - loss: 0.5964 - acc: 0.7231\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.5816 - acc: 0.7562\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.5664 - acc: 0.7769\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.5496 - acc: 0.7934\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 158us/step - loss: 0.5340 - acc: 0.7975\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 0.5182 - acc: 0.8140\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 0.5011 - acc: 0.8182\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 158us/step - loss: 0.4837 - acc: 0.8264\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 186us/step - loss: 0.4680 - acc: 0.8388\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 0.4506 - acc: 0.8306\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.4353 - acc: 0.8347\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.4199 - acc: 0.8388\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 199us/step - loss: 0.4053 - acc: 0.8223\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 0.3916 - acc: 0.8306\n",
      "61/61 [==============================] - 0s 8ms/step\n",
      "242/242 [==============================] - 0s 78us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7201 - acc: 0.5496\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 0.6951 - acc: 0.5496\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 0.6760 - acc: 0.5785\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 140us/step - loss: 0.6590 - acc: 0.6198\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 0.6442 - acc: 0.6570\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 150us/step - loss: 0.6303 - acc: 0.6777\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 0.6168 - acc: 0.6942\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 0.6039 - acc: 0.7107\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 0.5900 - acc: 0.7149\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.5754 - acc: 0.7314\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 156us/step - loss: 0.5626 - acc: 0.7438\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 0.5518 - acc: 0.7521\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.5381 - acc: 0.7727\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 155us/step - loss: 0.5249 - acc: 0.7810\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 148us/step - loss: 0.5126 - acc: 0.7893\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.5009 - acc: 0.7934\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 0.4883 - acc: 0.8017\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 0.4765 - acc: 0.8017\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 154us/step - loss: 0.4657 - acc: 0.8058\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 0.4548 - acc: 0.8058\n",
      "61/61 [==============================] - 1s 8ms/step\n",
      "242/242 [==============================] - 0s 64us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7317 - acc: 0.4008\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.7078 - acc: 0.4463\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 147us/step - loss: 0.6877 - acc: 0.5165\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 148us/step - loss: 0.6694 - acc: 0.5785\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 175us/step - loss: 0.6522 - acc: 0.6364\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 193us/step - loss: 0.6328 - acc: 0.7025\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 180us/step - loss: 0.6136 - acc: 0.7355\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 179us/step - loss: 0.5932 - acc: 0.7851\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.5721 - acc: 0.8017\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.5502 - acc: 0.8099\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 146us/step - loss: 0.5288 - acc: 0.8182\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 139us/step - loss: 0.5060 - acc: 0.8223\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 185us/step - loss: 0.4835 - acc: 0.8223\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.4627 - acc: 0.8347\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 142us/step - loss: 0.4434 - acc: 0.8306\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 173us/step - loss: 0.4273 - acc: 0.8430\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 154us/step - loss: 0.4123 - acc: 0.8471\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.4003 - acc: 0.8471\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 155us/step - loss: 0.3895 - acc: 0.8595\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.3808 - acc: 0.8595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 8ms/step\n",
      "242/242 [==============================] - 0s 63us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 6ms/step - loss: 0.7495 - acc: 0.4650\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 158us/step - loss: 0.7042 - acc: 0.5267\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 157us/step - loss: 0.6716 - acc: 0.5967\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 166us/step - loss: 0.6471 - acc: 0.6379\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 159us/step - loss: 0.6284 - acc: 0.7037\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 264us/step - loss: 0.6095 - acc: 0.7243\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 258us/step - loss: 0.5929 - acc: 0.7449\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 182us/step - loss: 0.5769 - acc: 0.7654\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 188us/step - loss: 0.5627 - acc: 0.7819\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 166us/step - loss: 0.5483 - acc: 0.7860\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 188us/step - loss: 0.5353 - acc: 0.7819\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 165us/step - loss: 0.5199 - acc: 0.7984\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 185us/step - loss: 0.5070 - acc: 0.8066\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 168us/step - loss: 0.4938 - acc: 0.8107\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 199us/step - loss: 0.4815 - acc: 0.8066\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 234us/step - loss: 0.4683 - acc: 0.8148\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 216us/step - loss: 0.4570 - acc: 0.8148\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 183us/step - loss: 0.4468 - acc: 0.8148\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 189us/step - loss: 0.4368 - acc: 0.8189\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 168us/step - loss: 0.4281 - acc: 0.8230\n",
      "60/60 [==============================] - 1s 9ms/step\n",
      "243/243 [==============================] - 0s 90us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 6ms/step - loss: 0.7151 - acc: 0.4156\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 207us/step - loss: 0.6927 - acc: 0.4815\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 180us/step - loss: 0.6744 - acc: 0.5597\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 161us/step - loss: 0.6566 - acc: 0.6173\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 153us/step - loss: 0.6399 - acc: 0.6914\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 169us/step - loss: 0.6229 - acc: 0.7490\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 174us/step - loss: 0.6043 - acc: 0.7819\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 182us/step - loss: 0.5837 - acc: 0.7942\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 172us/step - loss: 0.5613 - acc: 0.8230\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 189us/step - loss: 0.5369 - acc: 0.8313\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 172us/step - loss: 0.5110 - acc: 0.8354\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 170us/step - loss: 0.4842 - acc: 0.8477\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 160us/step - loss: 0.4602 - acc: 0.8477\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 156us/step - loss: 0.4378 - acc: 0.8519\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 167us/step - loss: 0.4171 - acc: 0.8601\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 156us/step - loss: 0.4008 - acc: 0.8642\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 184us/step - loss: 0.3869 - acc: 0.8642\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 194us/step - loss: 0.3752 - acc: 0.8683\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 206us/step - loss: 0.3665 - acc: 0.8683\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 169us/step - loss: 0.3576 - acc: 0.8683\n",
      "60/60 [==============================] - 1s 11ms/step\n",
      "243/243 [==============================] - 0s 98us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.6885 - acc: 0.5496\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.6743 - acc: 0.5950\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.6606 - acc: 0.6281\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.6475 - acc: 0.6488\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.6345 - acc: 0.6736\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.6219 - acc: 0.7025\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.6092 - acc: 0.7107\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.5972 - acc: 0.7355\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 73us/step - loss: 0.5849 - acc: 0.7314\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.5733 - acc: 0.7479\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.5609 - acc: 0.7479\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.5492 - acc: 0.7603\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5375 - acc: 0.7769\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.5258 - acc: 0.7769\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.5150 - acc: 0.7769\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 94us/step - loss: 0.5039 - acc: 0.7810\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.4924 - acc: 0.7934\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.4820 - acc: 0.8017\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.4714 - acc: 0.8099\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.4617 - acc: 0.8140\n",
      "61/61 [==============================] - 1s 10ms/step\n",
      "242/242 [==============================] - 0s 48us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.7216 - acc: 0.4091\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.7072 - acc: 0.4463\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.6949 - acc: 0.4835\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.6833 - acc: 0.5372\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.6720 - acc: 0.5909\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.6614 - acc: 0.6322\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6515 - acc: 0.6612\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.6418 - acc: 0.6860\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.6321 - acc: 0.7107\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.6224 - acc: 0.7603\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.6121 - acc: 0.7893\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.6024 - acc: 0.8017\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.5922 - acc: 0.8140\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.5815 - acc: 0.8140\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.5710 - acc: 0.8140\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.5596 - acc: 0.8140\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.5485 - acc: 0.8182\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5375 - acc: 0.8264\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.5259 - acc: 0.8306\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.5145 - acc: 0.8306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 10ms/step\n",
      "242/242 [==============================] - 0s 47us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.6738 - acc: 0.5496\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.6598 - acc: 0.5744\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.6471 - acc: 0.6074\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.6358 - acc: 0.6488\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.6245 - acc: 0.6777\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.6137 - acc: 0.7066\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 0.6029 - acc: 0.7231\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.5924 - acc: 0.7397\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 94us/step - loss: 0.5830 - acc: 0.7314\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.5722 - acc: 0.7438\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.5616 - acc: 0.7603\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.5516 - acc: 0.7810\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.5417 - acc: 0.7851\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.5315 - acc: 0.7851\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.5216 - acc: 0.7851\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.5119 - acc: 0.7934\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.5024 - acc: 0.7934\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.4938 - acc: 0.7975\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.4847 - acc: 0.7934\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.4759 - acc: 0.7975\n",
      "61/61 [==============================] - 1s 10ms/step\n",
      "242/242 [==============================] - 0s 48us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 0.7088 - acc: 0.5350\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.7018 - acc: 0.5556\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 0.6954 - acc: 0.5844\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 92us/step - loss: 0.6898 - acc: 0.6049\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.6842 - acc: 0.6132\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 0.6791 - acc: 0.6214\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 0.6739 - acc: 0.6214\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 90us/step - loss: 0.6685 - acc: 0.6379\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 0.6631 - acc: 0.6461\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.6574 - acc: 0.6420\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.6515 - acc: 0.6708\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 0.6448 - acc: 0.6749\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 0.6375 - acc: 0.6955\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.6298 - acc: 0.6955\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 0.6217 - acc: 0.7037\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 124us/step - loss: 0.6127 - acc: 0.7284\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 0.6031 - acc: 0.7490\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 0.5930 - acc: 0.7613\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 117us/step - loss: 0.5826 - acc: 0.7778\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 0.5711 - acc: 0.7901\n",
      "60/60 [==============================] - 1s 10ms/step\n",
      "243/243 [==============================] - 0s 42us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 0.6533 - acc: 0.5144\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 87us/step - loss: 0.6430 - acc: 0.5267\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.6340 - acc: 0.5761\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 0.6243 - acc: 0.6091\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 0.6154 - acc: 0.6379\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.6071 - acc: 0.6461\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 85us/step - loss: 0.5986 - acc: 0.6749\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.5904 - acc: 0.6914\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.5825 - acc: 0.7160\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 74us/step - loss: 0.5744 - acc: 0.7325\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 71us/step - loss: 0.5665 - acc: 0.7490\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.5582 - acc: 0.7490\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.5503 - acc: 0.7737\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.5419 - acc: 0.7860\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.5338 - acc: 0.7901\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 100us/step - loss: 0.5250 - acc: 0.8025\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.5162 - acc: 0.8066\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 85us/step - loss: 0.5067 - acc: 0.8066\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 0.4977 - acc: 0.8189\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.4883 - acc: 0.8354\n",
      "60/60 [==============================] - 1s 10ms/step\n",
      "243/243 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.6603 - acc: 0.7273\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.6563 - acc: 0.7479\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.6519 - acc: 0.7603\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.6472 - acc: 0.7769\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6427 - acc: 0.7851\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.6378 - acc: 0.7851\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.6331 - acc: 0.8058\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.6279 - acc: 0.8017\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.6226 - acc: 0.8099\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.6167 - acc: 0.8140\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.6113 - acc: 0.8140\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.6054 - acc: 0.8140\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.5991 - acc: 0.8099\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.5928 - acc: 0.8099\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.5862 - acc: 0.8099\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5794 - acc: 0.8140\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.5724 - acc: 0.8140\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5651 - acc: 0.8182\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.5577 - acc: 0.8182\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5502 - acc: 0.8223\n",
      "61/61 [==============================] - 1s 10ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 32us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.7077 - acc: 0.4917\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.6980 - acc: 0.5083\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.6885 - acc: 0.5248\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.6803 - acc: 0.5537\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.6718 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.6636 - acc: 0.5868\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6561 - acc: 0.6198\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.6479 - acc: 0.6612\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.6400 - acc: 0.6942\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.6325 - acc: 0.7025\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.6250 - acc: 0.7190\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.6173 - acc: 0.7273\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.6101 - acc: 0.7314\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.6025 - acc: 0.7355\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.5954 - acc: 0.7314\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.5878 - acc: 0.7397\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5807 - acc: 0.7479\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5734 - acc: 0.7521\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5662 - acc: 0.7562\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5592 - acc: 0.7603\n",
      "61/61 [==============================] - 1s 11ms/step\n",
      "242/242 [==============================] - 0s 33us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.6902 - acc: 0.6116\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.6848 - acc: 0.6157\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.6797 - acc: 0.6281\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 73us/step - loss: 0.6749 - acc: 0.6322\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.6702 - acc: 0.6446\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.6653 - acc: 0.6446\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.6609 - acc: 0.6488\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 102us/step - loss: 0.6563 - acc: 0.6529\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.6519 - acc: 0.6694\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 103us/step - loss: 0.6473 - acc: 0.6694\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.6426 - acc: 0.6901\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.6380 - acc: 0.7025\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.6331 - acc: 0.7107\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.6282 - acc: 0.7190\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.6232 - acc: 0.7355\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.6183 - acc: 0.7355\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.6132 - acc: 0.7438\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.6078 - acc: 0.7479\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.6024 - acc: 0.7645\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.5970 - acc: 0.7686\n",
      "61/61 [==============================] - 1s 16ms/step\n",
      "242/242 [==============================] - 0s 89us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 14ms/step - loss: 0.7964 - acc: 0.4074\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 142us/step - loss: 0.7831 - acc: 0.4074\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.7713 - acc: 0.4074\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.7603 - acc: 0.4198\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.7501 - acc: 0.4198\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.7406 - acc: 0.4198\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.7315 - acc: 0.4280\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 74us/step - loss: 0.7227 - acc: 0.4239\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.7143 - acc: 0.4486\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 0.7064 - acc: 0.4691\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.6988 - acc: 0.4815\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.6913 - acc: 0.4979\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.6839 - acc: 0.5267\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.6767 - acc: 0.5432\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.6693 - acc: 0.5802\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.6620 - acc: 0.6008\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.6545 - acc: 0.6132\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.6471 - acc: 0.6461\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.6394 - acc: 0.6584\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.6314 - acc: 0.6914\n",
      "60/60 [==============================] - 1s 13ms/step\n",
      "243/243 [==============================] - 0s 42us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 9ms/step - loss: 0.6750 - acc: 0.5062\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.6671 - acc: 0.5514\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.6597 - acc: 0.5802\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.6528 - acc: 0.6049\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.6457 - acc: 0.6379\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.6394 - acc: 0.6502\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.6328 - acc: 0.6749\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.6265 - acc: 0.6708\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.6202 - acc: 0.6914\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.6141 - acc: 0.6955\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.6075 - acc: 0.7243\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.6014 - acc: 0.7366\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.5948 - acc: 0.7449\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 73us/step - loss: 0.5884 - acc: 0.7695\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 96us/step - loss: 0.5819 - acc: 0.7860\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.5754 - acc: 0.7901\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5685 - acc: 0.7901\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5624 - acc: 0.7984\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5557 - acc: 0.8025\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.5493 - acc: 0.8066\n",
      "60/60 [==============================] - 1s 16ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/243 [===========>..................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "243/243 [==============================] - 0s 56us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sandbox/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "303/303 [==============================] - 3s 10ms/step - loss: 0.7116 - acc: 0.4653\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 317us/step - loss: 0.6797 - acc: 0.4818\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 324us/step - loss: 0.6508 - acc: 0.5974\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 335us/step - loss: 0.6141 - acc: 0.7129\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 407us/step - loss: 0.5693 - acc: 0.7624\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 406us/step - loss: 0.5172 - acc: 0.7921\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 415us/step - loss: 0.4699 - acc: 0.8119\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 443us/step - loss: 0.4295 - acc: 0.8350\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 403us/step - loss: 0.4011 - acc: 0.8647\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 482us/step - loss: 0.3825 - acc: 0.8581\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 449us/step - loss: 0.3676 - acc: 0.8548\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 377us/step - loss: 0.3569 - acc: 0.8581\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 446us/step - loss: 0.3491 - acc: 0.8614\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 555us/step - loss: 0.3432 - acc: 0.8647\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 523us/step - loss: 0.3371 - acc: 0.8581\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 392us/step - loss: 0.3319 - acc: 0.8614\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 389us/step - loss: 0.3296 - acc: 0.8680\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 576us/step - loss: 0.3251 - acc: 0.8713\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 773us/step - loss: 0.3224 - acc: 0.8713\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 543us/step - loss: 0.3194 - acc: 0.8746\n"
     ]
    }
   ],
   "source": [
    "# Adjust parameters\n",
    "batch_size = [10, 20, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                        epochs=20,verbose=1)\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                    n_jobs=1, cv=kfold)\n",
    "\n",
    "# Fit\n",
    "grid_result2 = grid.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEsCAYAAAA/5++aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8XdP9//HXWwxNVCUilJiqxn6p4LY1tOQrhlKpoWoqFdVG6YCqb9GqtLQovlHtj6+oiKJqCIoaG9JBS3sjMdVMyCQSxFBBxOf3x1pXzj0559597z33npPc9/PxOI9zz1p7+Jx99jmfu9dee21FBGZmZkUsU+8AzMxsyeGkYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYdTNJ60kKSeM6MM+IPM+I7ouscyRNlTS13nFYdZJG5f1naK2X3auTRt6opY+FkuZKulvSV+odXyPK22liveOw7iNpoqSl8gIuSePyPrxeHda9VHx3lq13AA3iJ/l5OWBjYG/gvyVtHRHfq19Y1ovdANwHzKp3IBUMq3cAVj9OGkBEjCp9LWkYcBdwrKTzI2JqPeKy3isiXgNeq3cclUTEM/WOweooInrtA4i0CSrW/TvXf7mkbAQwHngWmA+8DtwLHFJlGRPzMpYHfgw8AbwDjMv1KwMnAHcD04F3gTnATcA2bcQ8EVgdGAvMBv4D/B34XJ5mReBs4Pm8vkdL30eFZR4E3AO8CrwNPAb8CFih7L1HlceosuV9BrgOeDG/p2nARcCaHd1GBT7DTfJ2mJrnewn4K3BUhWmHAbcDr+T3+SRwJrByG3Etl+N6Js/zOPCNkum+CTyc94fppKPWZcqWtV5e1rgc7405hv8AfwN2rbD+lu09oqx8an70y5/xC/l9Pw38AFCVZbW735bEWekxsTyGCutZATgReAh4K6/nr8D+FaYt3SbrAb8H5uZt3AzsWWGe5YHvAg+Q9tW3cix/AHYu+n2v8JhaNt0qwBmk78F8UvKeUOVzajcmuum7k6ffmrRPv5G395+AbYFReflDa/mbGRE+0miD8nNp2+6FpGTyF1KzwUBgD+BySRtHxClVljUe+BRwG+kH46Vcvinws7y8P5J2unWALwK7SxoeEbdXWF5/0pf+DeAq0k5+IHCHpG1JO9kqwC2kH72DgKslTYuI+1q9SekS4GukH7zrgXnANsBpwDBJu0TEe8AU0g/iqaRkNK5kMRNLlnc4cDHph+wm0k6/IfB1YLikbSLihQ5so6okfQG4lvRjdXveFv2BLYD/IX1eLdMemV//J8/zEjCU9EM7XNL2ETGvwmp+T/oi3wosAPYDxkhaAHwSOIy0nSeQPrcfk344zqqwrI8B/wAeIX1GawAHALdJOjgirm7vPWfLAXcCa5K213ukJtUzgQ+xqLm1RdH9dl6edwSwbtlyprYVkKTlgTuAHUmJ9f+REtt+pH1vSEScXGHWdYF/khLa5aT99gDgD5J2joh7SqYdR9qXHwF+S/pBXxP4LPB50g9mW35C2k5bAL/M75eSZyStS9qf1yMlvNtJ/4TtCdwu6ciIuLiDMXXLd0fSdnn5y5O+u08DQ/Iy725nW3RerbPQkvSgypEGsDPwfn6sW1L+8Sr/aUwg/aAMLqubmNfxELBqhXlXrlK+FjATeKxazMD/UfIfLXBoLn8FuBn4UEnd53LdDWXLGpHLrwf6ltWNynXHVFj/xCrbcyPSf0dPV9gWOwELK8TQ5jZq47NblfQf4LvAjpW2Ycnf65K+iK8Dm5RNd0Fe/5gqcf0L6F9Svn5e56vAc6Xvk5Sw5pKOFpctKV+v5HM7u2w9TXnfeRX4SIXPZkTZ9FNz+a2lnxmwGunHbx6wXNk8ndpv29j2U1n8v/OTSuJatiyulpi3q7JNTi1b1m4tyyr7rrxPOgrpUyGmgQX3m3F52etVqZ+Y13NgWXl/0o//fGD1zsREDb87pH9qH8/L3Kts+mNKtu3Qot+pwt+9Wi9wSXqUbNhR+fEz0qHhe7n8fwsuZ988/Vcr7ICLfagFl3l+nnedCjH/B1iprLxP/gEIYP0Ky3sOeK6sbHKep3+F6fuQfgD/WWH91Xb80bn+C1Xqb8jbdqWSsk5tI+D4PN8vC0z7wzztzyvUDSAlk/m0bo5riWtYhXnuznVfq1B3aa5bt6RsvVw2r/xzy/Xjcv1hJWUjaDtpbFBhOZflus1qsd+2Md9UFk8aT5F+QDepMP0ReT1jK2yTqVT+wX0emFvy+iN5+nup0ATXgf2mZVuvV6Fui1x3bZV598r1R3cmplp+d4Dt8/R/rjBtH1Ly6Zak4eap5NT83PLF/itwSURcUTqRpHVIzRnDSM1IfcuWM7jK8v9ZbcWStif9Z7At6b+y5Ssss7w558mIeKO0ICIWSpoNrBgRz1ZY1QxSM0vLevuRviRzSSf8K4X3DqkJraht8/OOkj5VoX410g69ETCprK7qNqpim/x8W4Fpt8rPix2yR8SrkiYDO5DONzxYNklzheXNzM/l7wHSdoZ0tPh8Wd0D5Z9bNpHUzLUl6Ye/Pa9FxNMVyqfl5wGlhV3YbwuRtBKwATAjIh6vMEnLdt+yQt2UiFhYoXwai/YnIuJ1STcDw4EpksaTvqf3R8RbXYm/RMv6VpY0qkL9oPy8aTfE1NHvTss+/efyCfNvwd+Aj3cwhkKcNICIqPiLWUrS+qQftgGkHeNOUvPIQtJ/TYeR2tYrebHKMvchHdm8Teqt9QzpKOJ9Unv7jlWWWa1XzXvt1JV+3gNIh7iDWJQ0u2pgfj6hnek+XKGs4jZqQ//8PKPNqZKV83O17qst5f3LKyL1Yir3Xn5uq265CnWzq6y/5b2vXKW+XKVzL6Xr7tNS0MX9tqhOb1/afi/l15EdQEp+B7PofMvbkq4Dvh8R1bZvUS377y75UU3p/lurmDr63WnZ5u3tUzXnpFHc90gf7OERMa60QtJBpC9fRZGPGSs4jdSO2RQRj5Ut8yJS0uguLT94kyNiqzan7PgyV46I1zsyYxvbqJqWH5vBpN5LReL6KKknWbk1yqbrLqtXKf9oN66/0/ttB5Ru30pqsn0jYj65KVnS2qSjwxHAIaQE+LmuLJ9F8R0TEef3cEwd/e60TN/ePlVzvfqK8A7aID+Pr1DX2R/3DYB/V0gYy5B6X3SbiHiT9AP6X5JW6cCs71Pyn2yZlp5ZXf3yFtGyrt0LTDs5Pw8tr5DUn9TjpKWrcXfaKjfllGuJa3KFuq7qzH67EEBStc+5ldzk9gwwWNKGFSb57/z8QJHlFVzntIi4knTS/Cngs5IGtjMb5PdG5X24S/tvgZhq+d1p2ZaLfYb5c+u23w8njeKm5uehpYWSdiN1ievsMjeUtGbJ8kRqLvpEJ5fZEf9LOocyNv94tiJpgKTyo5CXgbWrLO/XpBProyVtVGF5y0uqVUK5jHQC+yhJO1RY11olL6/IcX1H0gZlk55GOqF5RUS8U6PYqlmZ1CX3A5KagK+Q/nO8oRvWOTU/Dy1bb1v77cv5eZ0OrGcsqbnz7NJkI2lV4JSSaTpF0iBJn6lQtSKwEqk5690Ci6r63iKimdSEt6+kr1WJY3NJq3Uyplp+d/5OuqZpB0l7lU3+bbrpfAa4eaojLgAOB67NJ7xmAJuR+mJfQ2rb7KjRpK6zk/MyF5B6RXyC1G12eA3irioixkraGjgaeEbSHaST7quQrinYgdQb6Jsls00ADswnACeRvhh/iYi/RMTj+cs2FnhU0u2kC+iWI31JP0fqjrpJDWKfK+lg0jmheyTdRuq2+xHS9RNr5/dAREyVdCzp2oEHJF2T49iRdALycVK7dHf7C/D1/ENzL4uu01gGOLKjTXoFdWa/nQB8Gbhe0q2knmXPR8TlbaznHNJR317Ag3m+fnk5qwG/iIi/deF9DAbuk/QY6b/saaTPek9SU8z5VToZlJtAOm9wcT7v8CYwLyJ+nesPJp24v0TSd4H7SU2ha5H2q81I+8xLnYipZt+diAhJR5DOhY6X1HKdxhakSwZuJ33GtVfr7lhL0oM2rgivMv12pB3qVdKFdX8jXSw0lMpXd05sb/mk9s8ppBPgc0n/bW5OlSs6abvb3lQqXKnbXiyknfwW0hfhXdJJtH8Cp7P4dQ2rAb8jnYBbWOV9b07q2thyRforLLqgbaeObqN2tt9/kS6qmpFjn03qUTKywrS7kk4Ev8qiq6h/QeUux21tr3FU77a52OdG66ufNyVdLdxy9fC9wG5V9otqXW6rfcbV9pmO7rd9gJ+TLrhr6cY9sb0YSBcWnpw/6/kl6zqowrQfbJMi+yvpJPqP8/uYkT+/WXm6g+hAN1zSeZ7H8jKi/L2QjhJOJv2wv5nfy3OkC3BHknoodjgmavzdydOXXhH+Bj1wRbjyis3MzNrlcxpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhPZo0JI2V9JKkR0rKVpF0l6Sn8vOAXC5J50t6WtJDFe7rYGZmPaynjzTGsfgY7ycCEyJiQ9J48yfm8t2BDfNjJHBhD8VoZmZV9GjSiIi/kMaHL7UX6S5s5Oe9S8p/G8l9QH9Ja2BmZnXTCHfuWz0iZgFExKyWWymS7oo1rWS66blsVvkCJI0kHY2w4oorbr3JJl2+MZyZWa8yadKkuRExqL3pGiFpVKMKZRXvGBURY4AxAE1NTdHc3NydcZmZLXUkPV9kukboPTW7pdkpP7+Uy6fT+ibsawEzezg2MzMr0QhJ4ybgsPz3YaT7J7eUfzX3otoGeK2lGcvMzOqjR5unJF1Fupn9qpKmA6cCZwLXSDoCeAH4cp78VmAP4GngLeDwnozVzMwW16NJIyIOqlI1rMK0AXyreyMyM7OOaITmKTMzW0I4aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYQ2TNCQdI+kRSY9KOjaXjZI0Q9KU/Nij3nGamfVmPXqP8GokbQZ8A/g08C5wu6Q/5urREXFO3YIzM7MPNETSADYF7ouItwAk/RnYp74hmZlZuUZpnnoE2EHSQEn9gD2AtXPdtyU9JGmspAGVZpY0UlKzpOY5c+b0VMxmZr1OQySNiHgMOAu4C7gdeBB4D7gQ+DgwBJgFnFtl/jER0RQRTYMGDeqZoM3MeqGGSBoAEXFJRGwVETsArwBPRcTsiFgYEe8DF5POeZiZWZ00TNKQtFp+XgfYF7hK0holk+xDasYyM7M6aZQT4QDjJQ0EFgDfiohXJV0uaQgQwFTgyHoGaGbW2zVM0oiIz1UoO7QesZiZWWUN0zxlZmaNz0nDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzAprmKQh6RhJj0h6VNKxuWwVSXdJeio/D6h3nGZmvVlDJA1JmwHfAD4NbAHsKWlD4ERgQkRsCEzIr83MrE4aImkAmwL3RcRbEfEe8GdgH2Av4LI8zWXA3nWKz8zMgGXrHUD2CPAzSQOB+cAeQDOwekTMAoiIWZJWa29BTzzxBEOHDm1Vtv/++3P00Ufz1ltvscceeyw2z4gRIxgxYgRz585lv/32W6z+qKOO4oADDmDatGkceuihi9Uff/zxDB8+nCeeeIIjjzxysfof/ehH7LzzzkyZMoVjjz12sfqf//znbLfddvz973/n5JNPXqz+vPPOY8iQIfzpT3/i9NNPX6z+oosuYuONN+bmm2/m3HPPXaz+8ssvZ+211+bqq6/mwgsvXKz+uuuuY9VVV2XcuHGMGzdusfpbb72Vfv36ccEFF3DNNdcsVj9x4kQAzjnnHG655ZZWdX379uW2224D4LTTTmPChAmt6gcOHMj48eMBOOmkk/jHP/7Rqn6ttdbiiiuuAODYY49lypQpreo32mgjxowZA8DIkSN58sknW9UPGTKE8847D4BDDjmE6dOnt6rfdtttOeOMMwD40pe+xMsvv9yqftiwYZxyyikA7L777syfP79V/Z577sn3v/99gMX2O/C+531vyd33qmmIpBERj0k6C7gLeBN4EHiv6PySRgIjAVZYYYVuidHMzEARUe8YFiPp58B04BhgaD7KWAOYGBEbtzVvU1NTNDc390SYZmZLDUmTIqKpveka5ZwGLU1PktYB9gWuAm4CDsuTHAb8oT7RmZkZNEjzVDY+n9NYAHwrIl6VdCZwjaQjgBeAL9c1QjOzXq5hkkZEfK5C2cvAsDqEY2ZmFTRM85SZmTU+Jw0zMyvMScPMzApz0jAzs8IKJQ1Ju3R3IGZm1viKHmncIelpSSdIGtStEZmZWcMqmjR2Av4FnAZMk/Q7STt2X1hmZtaICiWNiJgYEQcBg4FTgCbgHkmP5ftg+D4XZma9QKfHnpI0DBgFbAe8A1wDnBsRD9csuk5oa+yp0Xc9yS8nPNXpZR8zbEOO22WjTs9vZtaoio491amkIWkP4EjSEOZzSWNC7QasBXw3IhYfA7mHdHbAwgMuSsMiX33ktrUOycys4dV8wEJJH5X0Q0nPAbcA/YFDgLUj4pvABsBFwI87GbOZmTW4QmNPSRoP7Am8DVwBXBARj5ZOExELJf0OOLrmUZqZWUMoOmDhhsCxwOUR8WYb0z0M/HeXozIzs4ZUKGlExCcLTvcG6f7eZma2FCp6Rfiekr5dpe5b+cS4mZkt5Yo2T50CXF+lrm+uv7UmEfUC7vprZkuqokljE+CBKnVTgB/VJpze4bhdNqr6o++uv2bWyIp2uV0G+HCVupWA5WoTjpmZNbKiSeNB4CtV6r4CPFSbcMzMrJEVTRrnAvtKulbSrpI+IWkXSdcC+wBndzUQScdJelTSI5KukvQhSeMkPSdpSn4M6ep6zMys84p2ub1B0jHAz4B9c7GAN0nDhlQ7SV6IpMHAd4FPRMR8SdcAB+bqEyLiuq4s38zMaqPoiXAi4leSxpEGKBxIGnPq7+1c7NfRWPpKWgD0A2bWaLlmZlYjHbrda0S8ERF3RMTvIuLOWiWMiJgBnAO8AMwCXouIO3P1zyQ9JGm0pBUqzS9ppKRmSc1z5sypRUhmZlZBh5KGpAGSPi1ph/JHV4LI9+PYC/gYsCawoqRDgJNI3X0/BawC/KDS/BExJiKaIqJp0KCO31jwxskzmPzCPO5/7hW2P/Nubpw8o7NvxcxsqVZ0wMIPAWOB/UnnMirp04U4dgaei4g5eX3XA9tFxBW5/h1JlwLf78I6Krpx8gxOuv5h3l34PgAz5s3npOvTLUH23nJwrVdnZrZEK3qkcQowFDiMlDS+DXwd+BvwDGkE3K54AdhGUj9JAoYBj0laAyCX7Q080sX1LObsO55g/oKFrcrmL1jI2Xc8UetVmZkt8YomjS8BPwV+n1/fHxGXRsSOpGs4Pt+VICLifuA60lXnD+e4xgBXSno4l60KnN6V9VQyc978DpWbmfVmRXtPrQM8mu+ZsQBYsaRuLHApcExXAomIU4FTy4p36soyi1izf19mVEgQa/bv292rNjNb4hQ90niZRcOITAO2KKlblTRo4RLphN02pu9yrU/H9F2uDyfstnGdIjIza1xFjzTuA7YEbgPGA6dJWgl4DziedG5jidRysvt/rnuIdxe+z+D+fTlht419EtzMrIKiSeMsUhMVpPMKG5DOcfQhJZSjah9az9l7y8Fc9c8XAI8ua2bWlqLDiDQDzfnvN4Av5QvtVoiI17sxPrMe4/ucmLWv3aQhaXnS0cSJJVdpExHvAO90Y2xmPcr3OTFrX7snwiPiXdKV2u91fzhmZtbIivaeugvYtTsDMQ9nYmaNr+iJ8F8BV0haFriRNKhglE4QEc/WOLZexcOZmNmSoGjS+HN+/h5wXJVpujL2VK/X1nAmThpm1iiKJo3DuzUK83AmZrZEKNrl9rLuDqS383AmZrYk6ND9NKz7eDgTM1sSFL2fxth2JomIOKIG8fRaHs7EzJYERc9p7ERZbynSnfRWAublh3WRhzMxs0ZX9JzGepXK821e/w/4Sg1jMjOzBtWlcxoR8RdgNOk6DjMzW8rV4kT4s6Rh083MbClX9JxGRfkK8RHA9JpE082KjGK63ol/rFrnUUx7p5bhXd5d+D7bn3m3OyhYr1a099TdFYqXBzYCBgLf7Gogko4Dvk464f4w6YLCNUj3JV+FdP/wQ/MAip3S1iimZpV4eBez1oo2Ty0DqOzxBnA9MCwiLu5KEJIGA98FmiJiM9KQJAeSbv40OiI2BF4F3K3XelRbw7uY9UZFe08N7eY4IMXSV9ICoB9pUMSdgINz/WXAKODCHojFDPDwLmblGuKK8IiYAZwDvEBKFq8Bk4B5EdFyH4/pQMX2AEkjJTVLan766aeR9MFj0qRJTJo0qVXZqFGjAFhzzTU/KNt6660BGDlyZKtpZ86cyc0339yqbMyYMS3r/eAxfPhwAIYPH96qHGDMmDGtym6++WZmzpzZqmzkyJEA3PXzw7nmm9shiTXXXBOAUaNGLdHvaeutt/6gbEl7T6uttFzFfVbz5y2x72lp/Jz8nrr+nopSRPk1exUmkkYDq0bEoRXqLgdmR8T3C6918WUMAMYDB5AuFLw2vz41IjbI06wN3BoRm7e1rKampmhubu5sKHXnO8Q1lpZzGqVNVH2X68MZ+27ucxq2VJE0KSKa2puu6JHGF4E7q9TdAexdNLAqdgaei4g5EbGAdK5kO6C/Ug8tgLWAmV1cj1mH7L3lYM7Yd3OW75O+KoP793XCsF6taJfbwcC0KnVVm4064AVgG0n9gPnAMKAZuAfYj9SD6jDgD11cj1mHeXgXs0WKHmm8CmxQpW4DUk+qTouI+4HrSN1qH85xjQF+AHxP0tOkrr2XdGU9ZmbWNUWPNP4E/FDSzRExu6VQ0urAyaR7iHdJRJwKnFpW/Czw6a4u28zMaqNo0jgF+BfwlKRbWNQktSfwDvCj7gnPzMwaSdHrNKZK+hTwU2AXUlPRXOAGUg+n57svRDMzaxSFx56KiKnAV7svFDMza3SFToRLGiSp4qBNkjaStGptwzIzs0ZU9EjjAuAV4MgKdceRmqv2r1VQSzuPtmtmS6qiSeOzwLeq1N0J/Lo24fQOHm3XzJZURa/TGEAaD6qS10lHGmZmtpQrmjSmA5+pUvcZ0iCDZma2lCuaNK4DTpb0hdLC/PpE4JpaB2ZmZo2n6DmNnwI7ADdJehGYQbq476PAfcBPuic8MzNrJEUv7ntL0o7AoSy6uO9p0knwK0rueWG2xHKvNrP2FbqfRrsLkfpFxFs1iKfLlvT7aZiZ1UOt76dRbSVDJV0KvNiV5ZiZ2ZKh8DAiLSRtSBpO5FBgbdKAhdfXOC4zM2tAhZKGpJWBA0k3QvoMICCAs4CzIqLaNRxmZrYUqdo8JWkZSV+QdA3pOowLSUcWvwC2JyWO250wzMx6j7aONGYAqwFvAeOB3wJ/iojIRx5mZtaDivTwa0stevi1lTRWz8//BG4EJkYtulqZmVmntDVu3QEX/QPo/vvYt9V76nPAb4CtSFd8vyjpAknb1DoISRtLmlLyeF3SsZJGSZpRUr5HrddtZmbFVU0aEXFvRIwE1gAOId3u9RvAvcAU0onwAbUIIiKeiIghETEE2JrUJHZDrh7dUhcRt9ZifWZm1jntXqcREW9HxFUR8XlgHeAk4E3SifDxkv4k6eAaxjQMeMa3kDUzazwdurgvImZFxC8iYnPgU6QeVZ8ELq9hTAcCV5W8/rakhySNlVTxyEbSSEnNkprnzJlTw1DMzKxUp68Ij4hJEfEdYE3gS7UIRtLywBeBa3PRhcDHgSGkbr/nVollTEQ0RUTToEGDahGKmZlV0KVhRAAi4r2IuLEWwQC7Aw9ExOy87NkRsTAi3gcuBj5do/WYmVkndDlp1NhBlDRNSVqjpG4f4JEej8jMzD7Q4bGnuoukfqRh148sKf6FpCGknlpTy+rMzKyHNUzSyEOrDywrO7RO4ZiZWQWN1jxlZmYNrFNHGrnr6zakazXui4hXahqVmZk1pM7cT2NH0tXa7wMrAO9J2i8iJtQ6ODMzayydaZ4aDXwvIlYlDSNyFXBeTaMyM7OG1Nb9NH4laaUKVesBv4d0jQbprn3rdkt0ZmbWrhsnz2DyC/O4/7lX2P7Mu7lx8oxuW1dbRxrrA09KOqis/H5gtKRPSPo0cHIuMzOzHnbj5BmcdP3DvLvwfQBmzJvPSdc/3G2Jo61Rbr8AHA2cIWmCpJZB3L9JGm/qEeA+oB++fsLMrC7OvuMJ5i9Y2Kps/oKFnH3HE92yvjbPaUTEDcCmpGHRmyWdDsyOiO2BjwArR8Q2EfFst0RnZmZtmjlvfofKu6rI0OjzI+JE0rhP2wD/lrRnRLwZEW90S1RmZlbImv37dqi8q9pMGpKWyXfV2wKYGhE7Az8CLpJ0o6S1uyUqMzMr5ITdNqbvcn1alfVdrg8n7LZxt6yvrd5TnwQeBx4DJgPTJe0TEb8DNgGeBx6W9ANJDTMciZlZb7L3loM5Y9/NWb5P+jkf3L8vZ+y7OXtvObhb1tfWkcYYUrJYA1gZ+DXwW0kfiog3IuIYYEdgOPBgt0RnZmbt2nvLwWy5Tn8+87FVuPfEnbotYUDbSeMTwJh8T4s3SBfwrUi65SsAEfFgRHwWOKfbIjQzs4bRVrPSv4ATJc0D3ga+DbwMLNZTKiIu7Z7wzMyskbR1pHEEaWypfwEPAzsB++WrwM3MrBeqeqQREVOBHfLNkZaPiHk9FpWZmTWkdns95ZsjvdUDsZiZWYPzTZjMzKywhkga+QLCKSWP1yUdK2kVSXdJeio/D6h3rGZmvVlDJI2IeCIihkTEEGBrUnPYDcCJwISI2BCYkF+bmVmdNETSKDMMeCYingf2Ai7L5ZcBe9ctKjMza8ikcSDpboAAq0fELID8vFqlGSSNlNQsqXkco6kzAAAK7klEQVTOnDk9FKaZWe/TUElD0vLAF4FrOzJfRIyJiKaIaBo0aFD3BGdmZo2VNIDdgQciYnZ+PVvSGgD5+aW6RWZmZg2XNA5iUdMUwE3AYfnvw4A/9HhEZmb2AUVEvWMAIF95Pg1YPyJey2UDgWtIgyS+AHw5Il5pazlNTU3R3Nzc3eGamfW40Xc9yS8nPNXp+Y8ZtiHH7bJRxTpJkyKiqb1lNEzSqBUnDTOzjiuaNBqtecrMzBqYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFdYwSUNSf0nXSXpc0mOStpU0StIMSVPyY496x2lm1pstW+8ASvwSuD0i9pO0PNAP2A0YHRHn1Dc0MzODBkkakj4C7ACMAIiId4F3JdUzLDMzK9MozVPrA3OASyVNlvQbSSvmum9LekjSWEkDKs0saaSkZknNc+bM6bGgzcx6m0ZJGssCWwEXRsSWwH+AE4ELgY8DQ4BZwLmVZo6IMRHRFBFNgwYN6qGQzcx6n0ZJGtOB6RFxf359HbBVRMyOiIUR8T5wMfDpukVoZmaNkTQi4kVgmqSNc9Ew4N+S1iiZbB/gkR4PzszMPtAQJ8Kz7wBX5p5TzwKHA+dLGgIEMBU4sn7hmZlZwySNiJgCNJUVH1qPWMzMrLKGaJ4yM7Mlg5OGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU1TNKQ1F/SdZIel/SYpG0lrSLpLklP5ecB9Y7TzKw3a5ikAfwSuD0iNgG2AB4DTgQmRMSGwIT82szM6qQhkoakjwA7AJcARMS7ETEP2Au4LE92GbB3fSI0MzOAZesdQLY+MAe4VNIWwCTgGGD1iJgFEBGzJK1WaWZJI4GR+eWbkp7ogZi706rA3HoH0UC8PRbxtmjN26O1rmyPdYtMpIjo5PJrR1ITcB+wfUTcL+mXwOvAdyKif8l0r0bEUn9eQ1JzRDTVO45G4e2xiLdFa94erfXE9miI5ilgOjA9Iu7Pr68DtgJmS1oDID+/VKf4zMyMBkkaEfEiME3SxrloGPBv4CbgsFx2GPCHOoRnZmZZo5zTAPgOcKWk5YFngcNJSe0aSUcALwBfrmN8PWlMvQNoMN4ei3hbtObt0Vq3b4+GOKdhZmZLhoZonjIzsyWDk4aZmRXmpFFnktaWdE8eOuVRScfk8l47hIqkPpImS7olv/6YpPvztrg6n/fqFTy8ziKSjsvfkUckXSXpQ71p35A0VtJLkh4pKau4Lyg5X9LTkh6StFWt4nDSqL/3gOMjYlNgG+Bbkj5B7x5C5RjSMDItzgJG523xKnBEXaKqDw+vA0gaDHwXaIqIzYA+wIH0rn1jHPD5srJq+8LuwIb5MRK4sFZBOGnUWUTMiogH8t9vkH4UBtNLh1CRtBbwBeA3+bWAnUjX7kDv2hYeXqe1ZYG+kpYF+gGz6EX7RkT8BXilrLjavrAX8NtI7gP6t1zz1lVOGg1E0nrAlsD9lA2hAlQcQmUpdB7wP8D7+fVAYF5EvJdfTycl1d6gdHidyZJ+I2lFeuG+EREzgHNIXe9nAa+RhhvqrftGi2r7wmBgWsl0Nds2ThoNQtKHgfHAsRHxer3jqQdJewIvRcSk0uIKk/aWfuLLkkZGuDAitgT+Qy9oiqokt9XvBXwMWBNYkdQEU6637Bvt6bbvjZNGA5C0HClhXBkR1+fi3jiEyvbAFyVNBX5Pano4j3Ro3XIh6lrAzPqE1+M8vM4iOwPPRcSciFgAXA9sR+/dN1pU2xemA2uXTFezbeOkUWe5zf4S4LGI+N+Sql43hEpEnBQRa0XEeqSTnHdHxFeAe4D98mS9YluAh9cp8wKwjaR++TvTsi165b5Rotq+cBPw1dyLahvgtZZmrK7yFeF1JumzwF+Bh1nUjn8y6bzGNcA65CFUIqL8JNhSS9JQ4PsRsaek9UlHHqsAk4FDIuKdesbXUyQNIXUKWGx4HXrZviHpJ8ABpB6Hk4Gvk9rpe8W+IekqYChp+PPZwKnAjVTYF3Ji/TWpt9VbwOER0VyTOJw0zMysKDdPmZlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThq21JA0SlKUXOzVlWUNzcvr8HdE0ogcxwZdjaPAuiZKmtjd6zFr4aRhVtlQUj/4Rv+OHJ0fZj2ike4RbmYdFBH/rncM1rs0+n9RZp2xab6x1VuSZkn6aUszU75xz+h8I583Jb0o6WZJm7TMLGkU6SgDYEFuaoqS+hUlnSnpGUnv5GWMl7R6WRyrSrpS0uuSZuab4nyoI29E0jH55kvzJb0qqVnSPiX1rZqnWmKt8JhattxvSHpQ0tuS5kq6RNIqHYnNeicfadjS6EZgLHAGsBtwCmmIllHACsBKwOmkIbZXITXv3Cdpkzze029IA7wdAXwWWNiy4HxnuLuAIXn59wEr5/UMIA3v0OJy4CpgX2DbvP5XWZSQ2iTpK8C5wE9JQ830BT6ZY65m27LXawBXUnJTK0lnAscD5wMnkIbiOB3YTNJ2EbEQs2oiwg8/looH6Uc5gBPLyi8G3gD6V5inD+mGPm8Ax1VY1rJl038tl3+xjThG5Gl+UlZ+C/BkB97Pr4EH2plmIjCxSl1f4J/AU8DAXLYeKQn+uGza7XPMe9f7c/SjsR9unrKl0TVlr38PfBjYDEDS/kr3lZ5HGvzuP7l+Y9q3K/BiRNxUYNo/lr1+mDSwXFH/AoZI+pWknSX1KzpjHrDuMmAD4AsR8XKu2oXULH2lpGVbHqQBMl8n3SnQrConDVsaza7yerCk4cDVpOaag4HPAJ8i3SGvyPmGgcCMgnGUjzz7Dql5rKjfAkeRYrwDeEXS9fkOj+35KenWn/tGxJMl5S13dnsaWFD2+Ajp/ZlV5XMatjRanTSMeOlrSD/2RwFPR8SIlsp8E6yiJ4Hnko9YultEBHARcFG+c92upHMcV5MSSUWSDgZ+CHwtIiaWVbcccexKOr9S7uUKZWYf8JGGLY32L3t9IPAm8Ajp/MV7ZfWHks5tlGq5J0PfsvI7gY/mI5YeExGvRsTVpKa3qklL0rakTgBnRsS4CpPcReoUsE5ENFd4PNcd8dvSw0catjT6Ru5i+y9Sr6avA6MiYp6k24G9JY0mnZjeGvguMK9sGS3XPxwv6TZgYaSb2FwBfAO4StIZpHMBK+X1nBcRj9fqTUgaQzpB/w/SbTw3IiW4O6tM/xFSz7HHgZvzHdtavBMRkyPiGUlnAb/OdwT8M/A26daguwC/iYh7avUebOnjpGFLo72AX5G62r5G6k56Wq67mPQD+TXgSFJiGQ7cULaMW4ALSN1xfwyIdNOyBZJ2JXWbHZmfXwbuZfFzGF11L+lOfYeSuvXOJCWtal12VyGds1gN+HtZ3fOknlNExMmSHgO+lR8BTAMmkHpamVXlO/eZmVlhPqdhZmaFuXnKrA4k9SE1eVXzfkS831PxmBXlIw2z+pjA4tdJlD7G1i80s+p8TsOsDnLPpZXamGRuREztoXDMCnPSMDOzwtw8ZWZmhTlpmJlZYU4aZmZWmJOGmZkV9v8BU4vz+jv7ToYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.16%  Stdev: 3.44% Params: {'batch_size': 10}\n",
      "Accuracy: 82.18%  Stdev: 4.03% Params: {'batch_size': 20}\n",
      "Accuracy: 80.20%  Stdev: 3.16% Params: {'batch_size': 50}\n",
      "Accuracy: 72.61%  Stdev: 3.25% Params: {'batch_size': 100}\n",
      "\n",
      "Best: 84.16% using {'batch_size': 10}\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid_result2\n",
    "\n",
    "# Plot the accuracy of all the things tried in this gridsearch\n",
    "var = [x['batch_size'] for x in params]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('batch_size', fontsize=16)\n",
    "ax.set_ylabel('% Accuracy', fontsize=16)\n",
    "ax.set_ylim(60,100)\n",
    "ax.axhline(80, color='k', linestyle='--', linewidth=1)\n",
    "ax.axhline(90, color='k', linestyle='--')\n",
    "ax = plt.errorbar(var, means, yerr=stds, fmt='o', capsize=8)\n",
    "plt.title('Parameter combinations tested', fontsize=20, y=1.05)\n",
    "plt.show()\n",
    "\n",
    "# Report Results\n",
    "means = [x*100 for x in grid_result.cv_results_['mean_test_score']]\n",
    "stds = [x*100 for x in grid_result.cv_results_['std_test_score']]\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Accuracy: {mean:.2f}%  Stdev: {stdev:.2f}% Params: {param}')\n",
    "print()\n",
    "print(f\"Best: {grid_result.best_score_*100:.2f}% using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll stick with a batch size of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tune learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is fed to an optimizer instance, so I have to import that library\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.7750 - acc: 0.4587\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.7261 - acc: 0.4752\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 360us/step - loss: 0.6942 - acc: 0.5124\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.6702 - acc: 0.5909\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 377us/step - loss: 0.6473 - acc: 0.6612\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.6249 - acc: 0.6983\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.6006 - acc: 0.7397\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.5748 - acc: 0.7603\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 369us/step - loss: 0.5484 - acc: 0.7810\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5223 - acc: 0.7934\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 361us/step - loss: 0.4973 - acc: 0.8058\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 362us/step - loss: 0.4744 - acc: 0.8099\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.4523 - acc: 0.8099\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.4323 - acc: 0.8140\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 449us/step - loss: 0.4135 - acc: 0.8223\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 358us/step - loss: 0.3965 - acc: 0.8223\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3801 - acc: 0.8264\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.3668 - acc: 0.8306\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3524 - acc: 0.8430\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3417 - acc: 0.8471\n",
      "61/61 [==============================] - 1s 14ms/step\n",
      "242/242 [==============================] - 0s 223us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.7133 - acc: 0.4504\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.6799 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.6563 - acc: 0.5702\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.6376 - acc: 0.6240\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.6214 - acc: 0.6570\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.6052 - acc: 0.6860\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.5904 - acc: 0.6942\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.5714 - acc: 0.7066\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.5508 - acc: 0.7149\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.5281 - acc: 0.7397\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.5047 - acc: 0.7479\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4822 - acc: 0.7645\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.4636 - acc: 0.7727\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.4462 - acc: 0.7769\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.4319 - acc: 0.7769\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4191 - acc: 0.7893\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.4108 - acc: 0.8017\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.4013 - acc: 0.8058\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.3931 - acc: 0.8140\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3865 - acc: 0.8182\n",
      "61/61 [==============================] - 1s 13ms/step\n",
      "242/242 [==============================] - 0s 272us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 3s 12ms/step - loss: 0.6500 - acc: 0.6612\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 371us/step - loss: 0.6197 - acc: 0.7231\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 353us/step - loss: 0.5862 - acc: 0.7645\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.5514 - acc: 0.7975\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 365us/step - loss: 0.5145 - acc: 0.7934\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 370us/step - loss: 0.4804 - acc: 0.8099\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 376us/step - loss: 0.4527 - acc: 0.8182\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.4278 - acc: 0.8182\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 369us/step - loss: 0.4098 - acc: 0.8306\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.3951 - acc: 0.8347\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 341us/step - loss: 0.3829 - acc: 0.8430\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 331us/step - loss: 0.3744 - acc: 0.8471\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.3661 - acc: 0.8430\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.3595 - acc: 0.8430\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.3546 - acc: 0.8430\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 370us/step - loss: 0.3496 - acc: 0.8430\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 359us/step - loss: 0.3453 - acc: 0.8388\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3428 - acc: 0.8388\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 358us/step - loss: 0.3403 - acc: 0.8471\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.3374 - acc: 0.8512\n",
      "61/61 [==============================] - 1s 14ms/step\n",
      "242/242 [==============================] - 0s 273us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 0.6529 - acc: 0.6008\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 447us/step - loss: 0.6032 - acc: 0.7078\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 386us/step - loss: 0.5636 - acc: 0.7654\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 386us/step - loss: 0.5303 - acc: 0.7778\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 378us/step - loss: 0.5010 - acc: 0.7778\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 371us/step - loss: 0.4756 - acc: 0.7942\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 360us/step - loss: 0.4523 - acc: 0.8230\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 431us/step - loss: 0.4313 - acc: 0.8230\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 409us/step - loss: 0.4157 - acc: 0.8189\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 421us/step - loss: 0.4007 - acc: 0.8354\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 369us/step - loss: 0.3883 - acc: 0.8313\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 384us/step - loss: 0.3780 - acc: 0.8354\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 348us/step - loss: 0.3684 - acc: 0.8395\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 378us/step - loss: 0.3601 - acc: 0.8436\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 349us/step - loss: 0.3537 - acc: 0.8519\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 396us/step - loss: 0.3462 - acc: 0.8560\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 436us/step - loss: 0.3402 - acc: 0.8601\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 492us/step - loss: 0.3357 - acc: 0.8642\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 499us/step - loss: 0.3308 - acc: 0.8724\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 407us/step - loss: 0.3262 - acc: 0.8724\n",
      "60/60 [==============================] - 1s 14ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 307us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 0.7931 - acc: 0.4733\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 371us/step - loss: 0.7206 - acc: 0.5514\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 360us/step - loss: 0.6730 - acc: 0.6255\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 384us/step - loss: 0.6347 - acc: 0.6872\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 378us/step - loss: 0.6016 - acc: 0.7243\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 392us/step - loss: 0.5689 - acc: 0.7613\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 353us/step - loss: 0.5375 - acc: 0.7901\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 392us/step - loss: 0.5065 - acc: 0.8107\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 358us/step - loss: 0.4746 - acc: 0.8107\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 383us/step - loss: 0.4430 - acc: 0.8313\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 370us/step - loss: 0.4145 - acc: 0.8395\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 424us/step - loss: 0.3906 - acc: 0.8436\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 395us/step - loss: 0.3679 - acc: 0.8601\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 397us/step - loss: 0.3542 - acc: 0.8683\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 441us/step - loss: 0.3425 - acc: 0.8724\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 437us/step - loss: 0.3318 - acc: 0.8724\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 481us/step - loss: 0.3251 - acc: 0.8601\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 403us/step - loss: 0.3188 - acc: 0.8642\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 485us/step - loss: 0.3135 - acc: 0.8724\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 383us/step - loss: 0.3082 - acc: 0.8765\n",
      "60/60 [==============================] - 1s 14ms/step\n",
      "243/243 [==============================] - 0s 297us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 0.6129 - acc: 0.6570\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 398us/step - loss: 0.4182 - acc: 0.8140\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 367us/step - loss: 0.3322 - acc: 0.8595\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 339us/step - loss: 0.2991 - acc: 0.8760\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 368us/step - loss: 0.2625 - acc: 0.8967\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 367us/step - loss: 0.2473 - acc: 0.8967\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 371us/step - loss: 0.2350 - acc: 0.9174\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 380us/step - loss: 0.2257 - acc: 0.9132\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 374us/step - loss: 0.2160 - acc: 0.9132\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.2013 - acc: 0.9256\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 376us/step - loss: 0.1850 - acc: 0.9339\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.1881 - acc: 0.9298\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 440us/step - loss: 0.1641 - acc: 0.9339\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.1585 - acc: 0.9380\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 370us/step - loss: 0.1545 - acc: 0.9504\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 379us/step - loss: 0.1786 - acc: 0.9256\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 368us/step - loss: 0.1525 - acc: 0.9463\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.1371 - acc: 0.9463\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.1350 - acc: 0.9463\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 382us/step - loss: 0.1111 - acc: 0.9587\n",
      "61/61 [==============================] - 1s 14ms/step\n",
      "242/242 [==============================] - 0s 271us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 0.6410 - acc: 0.6570\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 371us/step - loss: 0.4898 - acc: 0.7934\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.3968 - acc: 0.8306\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.3534 - acc: 0.8636\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 374us/step - loss: 0.3213 - acc: 0.8678\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 368us/step - loss: 0.3205 - acc: 0.8636\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 400us/step - loss: 0.2946 - acc: 0.8926\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 450us/step - loss: 0.2881 - acc: 0.8884\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.2687 - acc: 0.8926\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2421 - acc: 0.9256\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 401us/step - loss: 0.2284 - acc: 0.9091\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 351us/step - loss: 0.2258 - acc: 0.8926\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.2084 - acc: 0.9050\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.2040 - acc: 0.9215\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 374us/step - loss: 0.1771 - acc: 0.9380\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 383us/step - loss: 0.1710 - acc: 0.9421\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.1494 - acc: 0.9545\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.1409 - acc: 0.9587\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.1292 - acc: 0.9463\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 406us/step - loss: 0.1198 - acc: 0.9669\n",
      "61/61 [==============================] - 1s 15ms/step\n",
      "242/242 [==============================] - 0s 257us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 3s 12ms/step - loss: 0.6122 - acc: 0.6405\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.4026 - acc: 0.8264\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.3436 - acc: 0.8595\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.3140 - acc: 0.8471\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.2996 - acc: 0.8802\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.2826 - acc: 0.8760\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.2542 - acc: 0.9050\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2437 - acc: 0.9008\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 539us/step - loss: 0.2336 - acc: 0.9050\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2384 - acc: 0.8884\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.2366 - acc: 0.9008\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 382us/step - loss: 0.1915 - acc: 0.9339\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 378us/step - loss: 0.1730 - acc: 0.9421\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 363us/step - loss: 0.1687 - acc: 0.9463\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 371us/step - loss: 0.1571 - acc: 0.9421\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 383us/step - loss: 0.1428 - acc: 0.9669\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 400us/step - loss: 0.1325 - acc: 0.9587\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 367us/step - loss: 0.1220 - acc: 0.9587\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.1085 - acc: 0.9669\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.1109 - acc: 0.9587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 15ms/step\n",
      "242/242 [==============================] - 0s 273us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 0.6215 - acc: 0.6502\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 374us/step - loss: 0.4494 - acc: 0.8436\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 370us/step - loss: 0.3784 - acc: 0.8477\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 363us/step - loss: 0.3502 - acc: 0.8642\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 398us/step - loss: 0.3252 - acc: 0.8519\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 383us/step - loss: 0.3260 - acc: 0.8683\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 384us/step - loss: 0.2930 - acc: 0.9012\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 380us/step - loss: 0.2772 - acc: 0.8848\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 387us/step - loss: 0.2614 - acc: 0.9136\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 367us/step - loss: 0.2557 - acc: 0.9136\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 379us/step - loss: 0.2253 - acc: 0.9053\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 395us/step - loss: 0.2156 - acc: 0.9300\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 372us/step - loss: 0.1961 - acc: 0.9342\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 393us/step - loss: 0.1891 - acc: 0.9259\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 375us/step - loss: 0.1768 - acc: 0.9259\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 395us/step - loss: 0.1763 - acc: 0.9300\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 403us/step - loss: 0.1400 - acc: 0.9547\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 396us/step - loss: 0.1409 - acc: 0.9383\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 412us/step - loss: 0.1164 - acc: 0.9630\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 387us/step - loss: 0.1044 - acc: 0.9588\n",
      "60/60 [==============================] - 1s 15ms/step\n",
      "243/243 [==============================] - 0s 257us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.5903 - acc: 0.7202\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 393us/step - loss: 0.3886 - acc: 0.8436\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 379us/step - loss: 0.3395 - acc: 0.8642\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 386us/step - loss: 0.3145 - acc: 0.8807\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 430us/step - loss: 0.3081 - acc: 0.8724\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 389us/step - loss: 0.2840 - acc: 0.8848\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 489us/step - loss: 0.2614 - acc: 0.9012\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 486us/step - loss: 0.2486 - acc: 0.9012\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 472us/step - loss: 0.2348 - acc: 0.8971\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 469us/step - loss: 0.2252 - acc: 0.9218\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 874us/step - loss: 0.2149 - acc: 0.9177\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 474us/step - loss: 0.1995 - acc: 0.9218\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 483us/step - loss: 0.1714 - acc: 0.9383\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 556us/step - loss: 0.1740 - acc: 0.9342\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 530us/step - loss: 0.1485 - acc: 0.9465\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 510us/step - loss: 0.1463 - acc: 0.9465\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 522us/step - loss: 0.1424 - acc: 0.9424\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 571us/step - loss: 0.1279 - acc: 0.9424\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 476us/step - loss: 0.1070 - acc: 0.9712\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 487us/step - loss: 0.1136 - acc: 0.9712\n",
      "60/60 [==============================] - 1s 16ms/step\n",
      "243/243 [==============================] - 0s 273us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 3s 11ms/step - loss: 0.5300 - acc: 0.8099\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3710 - acc: 0.8388\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.3347 - acc: 0.8430\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.3398 - acc: 0.8430\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 452us/step - loss: 0.3629 - acc: 0.8471\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.3529 - acc: 0.8636\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.3120 - acc: 0.8595\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.2794 - acc: 0.8926\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2734 - acc: 0.8843\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2864 - acc: 0.8719\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2283 - acc: 0.8926\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2389 - acc: 0.8760\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2199 - acc: 0.8926\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.2271 - acc: 0.8843\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3039 - acc: 0.8471\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.2741 - acc: 0.8884\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 459us/step - loss: 0.3802 - acc: 0.8843\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2998 - acc: 0.8678\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.2455 - acc: 0.9091\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 474us/step - loss: 0.2880 - acc: 0.8926\n",
      "61/61 [==============================] - 1s 18ms/step\n",
      "242/242 [==============================] - 0s 259us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 0.5120 - acc: 0.7479\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 366us/step - loss: 0.3995 - acc: 0.8223\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.4300 - acc: 0.8264\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.3946 - acc: 0.8099\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.3359 - acc: 0.8512\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 377us/step - loss: 0.3113 - acc: 0.8512\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 400us/step - loss: 0.2790 - acc: 0.8595\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2788 - acc: 0.8843\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3325 - acc: 0.8719\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3358 - acc: 0.8554\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.2967 - acc: 0.8760\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.2590 - acc: 0.8802\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.2680 - acc: 0.8678\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.3737 - acc: 0.8512\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.2456 - acc: 0.8884\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.2219 - acc: 0.9091\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 407us/step - loss: 0.2735 - acc: 0.9050\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 401us/step - loss: 0.2037 - acc: 0.9298\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.2523 - acc: 0.9091\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 411us/step - loss: 0.2647 - acc: 0.9008\n",
      "61/61 [==============================] - 1s 19ms/step\n",
      "242/242 [==============================] - 0s 277us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 4s 15ms/step - loss: 0.5319 - acc: 0.7893\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.4180 - acc: 0.8347\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 441us/step - loss: 0.4625 - acc: 0.8058\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.4173 - acc: 0.8388\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.3653 - acc: 0.8140\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 487us/step - loss: 0.3445 - acc: 0.8264\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3514 - acc: 0.8636\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3766 - acc: 0.8223\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 398us/step - loss: 0.3799 - acc: 0.8017\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 543us/step - loss: 0.3580 - acc: 0.8471\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 534us/step - loss: 0.3380 - acc: 0.8471\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.3252 - acc: 0.8554\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.2928 - acc: 0.8884\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 562us/step - loss: 0.2912 - acc: 0.8595\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.2988 - acc: 0.8760\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2604 - acc: 0.8760\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.2374 - acc: 0.8967\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.2666 - acc: 0.8926\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 561us/step - loss: 0.2523 - acc: 0.8967\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 451us/step - loss: 0.3796 - acc: 0.8471\n",
      "61/61 [==============================] - 1s 20ms/step\n",
      "242/242 [==============================] - 0s 325us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 12ms/step - loss: 0.5505 - acc: 0.7449\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 443us/step - loss: 0.4413 - acc: 0.8272\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 465us/step - loss: 0.3696 - acc: 0.8519\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 497us/step - loss: 0.3439 - acc: 0.8642\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 449us/step - loss: 0.3990 - acc: 0.8642\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 419us/step - loss: 0.3578 - acc: 0.8642\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 381us/step - loss: 0.3251 - acc: 0.8601\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 388us/step - loss: 0.3313 - acc: 0.8765\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 402us/step - loss: 0.3012 - acc: 0.8889\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 415us/step - loss: 0.2170 - acc: 0.9136\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 422us/step - loss: 0.2750 - acc: 0.8971\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 396us/step - loss: 0.2377 - acc: 0.9012\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 435us/step - loss: 0.2065 - acc: 0.9012\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 415us/step - loss: 0.2724 - acc: 0.8971\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 418us/step - loss: 0.2455 - acc: 0.8930\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 406us/step - loss: 0.2254 - acc: 0.8930\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 449us/step - loss: 0.2305 - acc: 0.9095\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 483us/step - loss: 0.2032 - acc: 0.9259\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 501us/step - loss: 0.1974 - acc: 0.9259\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 440us/step - loss: 0.2320 - acc: 0.9300\n",
      "60/60 [==============================] - 1s 20ms/step\n",
      "243/243 [==============================] - 0s 261us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 13ms/step - loss: 0.5188 - acc: 0.7819\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 399us/step - loss: 0.4353 - acc: 0.8477\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 425us/step - loss: 0.3993 - acc: 0.8560\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 409us/step - loss: 0.3229 - acc: 0.8477\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 409us/step - loss: 0.3430 - acc: 0.8642\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 448us/step - loss: 0.3228 - acc: 0.8683\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 437us/step - loss: 0.2840 - acc: 0.8724\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 417us/step - loss: 0.3496 - acc: 0.8313\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 492us/step - loss: 0.2967 - acc: 0.8930\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 528us/step - loss: 0.2900 - acc: 0.8477\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 491us/step - loss: 0.2370 - acc: 0.8971\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 546us/step - loss: 0.2889 - acc: 0.8930\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 496us/step - loss: 0.4354 - acc: 0.8683\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 507us/step - loss: 0.3624 - acc: 0.8436\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 484us/step - loss: 0.2679 - acc: 0.8642\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 485us/step - loss: 0.2866 - acc: 0.8519\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 434us/step - loss: 0.2450 - acc: 0.8724\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 457us/step - loss: 0.2147 - acc: 0.8889\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 438us/step - loss: 0.2034 - acc: 0.8971\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 455us/step - loss: 0.2359 - acc: 0.8765\n",
      "60/60 [==============================] - 1s 20ms/step\n",
      "243/243 [==============================] - 0s 307us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 3s 12ms/step - loss: 1.9254 - acc: 0.6860\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 485us/step - loss: 1.1855 - acc: 0.7810\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 511us/step - loss: 1.5479 - acc: 0.6983\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 430us/step - loss: 1.3684 - acc: 0.6983\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 438us/step - loss: 1.3640 - acc: 0.6942\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 389us/step - loss: 1.0766 - acc: 0.6364\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 415us/step - loss: 1.3643 - acc: 0.7686\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 459us/step - loss: 1.5303 - acc: 0.7066\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 480us/step - loss: 2.7700 - acc: 0.7975\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 476us/step - loss: 3.0516 - acc: 0.7893\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 528us/step - loss: 3.0501 - acc: 0.7893\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 478us/step - loss: 3.0511 - acc: 0.7893\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 412us/step - loss: 3.0520 - acc: 0.7893\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 491us/step - loss: 3.0510 - acc: 0.7893\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 513us/step - loss: 3.0515 - acc: 0.7893\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 452us/step - loss: 3.0502 - acc: 0.7893\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 434us/step - loss: 3.0516 - acc: 0.7893\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 450us/step - loss: 3.0508 - acc: 0.7893\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 414us/step - loss: 3.0547 - acc: 0.7893\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 532us/step - loss: 3.0532 - acc: 0.7893\n",
      "61/61 [==============================] - 1s 18ms/step\n",
      "242/242 [==============================] - 0s 282us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 4s 15ms/step - loss: 1.3202 - acc: 0.4876\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.8808 - acc: 0.5207\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8921 - acc: 0.4876\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 743us/step - loss: 1.7176 - acc: 0.5455\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1760 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.7546 - acc: 0.4835\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.7946 - acc: 0.5041\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 627us/step - loss: 1.1482 - acc: 0.5207\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 639us/step - loss: 1.1833 - acc: 0.5413\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 621us/step - loss: 1.1799 - acc: 0.5455\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 730us/step - loss: 1.1818 - acc: 0.5372\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 622us/step - loss: 1.1790 - acc: 0.5537\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 719us/step - loss: 1.1965 - acc: 0.5165\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 575us/step - loss: 1.1886 - acc: 0.5372\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.1826 - acc: 0.5702\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 549us/step - loss: 1.1758 - acc: 0.5702\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 597us/step - loss: 1.1837 - acc: 0.5165\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 552us/step - loss: 1.2307 - acc: 0.5661\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 570us/step - loss: 1.2280 - acc: 0.5496\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 767us/step - loss: 1.1898 - acc: 0.5207\n",
      "61/61 [==============================] - 2s 29ms/step\n",
      "242/242 [==============================] - 0s 430us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 4s 16ms/step - loss: 2.8843 - acc: 0.7314\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 457us/step - loss: 4.9319 - acc: 0.6860\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 452us/step - loss: 5.1491 - acc: 0.6694\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 520us/step - loss: 2.9903 - acc: 0.7479\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 590us/step - loss: 1.9399 - acc: 0.7273\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 425us/step - loss: 1.9007 - acc: 0.7562\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 476us/step - loss: 1.9021 - acc: 0.8471\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 526us/step - loss: 2.1609 - acc: 0.8182\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 535us/step - loss: 1.5604 - acc: 0.7438\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 512us/step - loss: 1.1675 - acc: 0.6570\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 820us/step - loss: 1.1668 - acc: 0.6736\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 618us/step - loss: 1.1729 - acc: 0.6777\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 605us/step - loss: 1.1588 - acc: 0.6777\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 510us/step - loss: 1.1808 - acc: 0.7025\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1681 - acc: 0.6818\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 616us/step - loss: 1.1837 - acc: 0.6653\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 510us/step - loss: 1.1732 - acc: 0.6694\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 498us/step - loss: 1.1855 - acc: 0.6570\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 516us/step - loss: 1.1633 - acc: 0.6777\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 487us/step - loss: 1.1684 - acc: 0.6860\n",
      "61/61 [==============================] - 1s 22ms/step\n",
      "242/242 [==============================] - 0s 324us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 14ms/step - loss: 1.1788 - acc: 0.5185\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 464us/step - loss: 0.9673 - acc: 0.6502\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 540us/step - loss: 0.8329 - acc: 0.5597\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 450us/step - loss: 0.8528 - acc: 0.6708\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 567us/step - loss: 0.9500 - acc: 0.6502\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 586us/step - loss: 1.1653 - acc: 0.5556 0s - loss: 0.7477 - acc: 0.60\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 503us/step - loss: 0.8626 - acc: 0.5144\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 526us/step - loss: 0.8280 - acc: 0.5226\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 630us/step - loss: 0.8223 - acc: 0.5062\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 660us/step - loss: 0.8249 - acc: 0.5432\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 556us/step - loss: 0.8161 - acc: 0.5185\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 523us/step - loss: 0.8291 - acc: 0.4938\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 518us/step - loss: 0.8276 - acc: 0.4362\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 771us/step - loss: 0.8411 - acc: 0.5103\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 667us/step - loss: 0.8665 - acc: 0.4733\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 554us/step - loss: 0.8249 - acc: 0.5021\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 0.8208 - acc: 0.4979\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 605us/step - loss: 0.8336 - acc: 0.5103\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 519us/step - loss: 0.8782 - acc: 0.4774\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 526us/step - loss: 0.8811 - acc: 0.5185\n",
      "60/60 [==============================] - 1s 23ms/step\n",
      "243/243 [==============================] - 0s 490us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 14ms/step - loss: 2.5373 - acc: 0.5103\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 483us/step - loss: 0.8715 - acc: 0.4650\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 456us/step - loss: 0.7802 - acc: 0.5926\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 439us/step - loss: 0.9249 - acc: 0.6091\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 454us/step - loss: 0.9010 - acc: 0.5802\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 457us/step - loss: 0.7769 - acc: 0.5514\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 414us/step - loss: 0.7243 - acc: 0.4856\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 463us/step - loss: 0.7093 - acc: 0.5021\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 434us/step - loss: 0.7113 - acc: 0.5062\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 449us/step - loss: 0.7145 - acc: 0.4979\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 454us/step - loss: 0.7258 - acc: 0.4979\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 493us/step - loss: 0.7065 - acc: 0.5144\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 493us/step - loss: 0.6887 - acc: 0.5226\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 499us/step - loss: 0.7078 - acc: 0.4815\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 469us/step - loss: 0.7096 - acc: 0.5144\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 478us/step - loss: 0.7077 - acc: 0.5103\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 436us/step - loss: 0.7042 - acc: 0.4979\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 439us/step - loss: 0.7480 - acc: 0.4650\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 462us/step - loss: 0.7190 - acc: 0.4938\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 486us/step - loss: 0.6904 - acc: 0.5391\n",
      "60/60 [==============================] - 1s 20ms/step\n",
      "243/243 [==============================] - 0s 290us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sandbox/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "303/303 [==============================] - 3s 11ms/step - loss: 0.7116 - acc: 0.4653\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 456us/step - loss: 0.6797 - acc: 0.4818\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 561us/step - loss: 0.6508 - acc: 0.5974\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 498us/step - loss: 0.6141 - acc: 0.7129\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 454us/step - loss: 0.5693 - acc: 0.7624\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 503us/step - loss: 0.5172 - acc: 0.7921\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 456us/step - loss: 0.4699 - acc: 0.8119\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 488us/step - loss: 0.4295 - acc: 0.8350\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 466us/step - loss: 0.4011 - acc: 0.8647\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 487us/step - loss: 0.3825 - acc: 0.8581\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 490us/step - loss: 0.3676 - acc: 0.8548\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 498us/step - loss: 0.3569 - acc: 0.8581\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 476us/step - loss: 0.3490 - acc: 0.8614\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 471us/step - loss: 0.3433 - acc: 0.8647\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 472us/step - loss: 0.3371 - acc: 0.8581\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 473us/step - loss: 0.3318 - acc: 0.8614\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 492us/step - loss: 0.3296 - acc: 0.8680\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 475us/step - loss: 0.3251 - acc: 0.8713\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 492us/step - loss: 0.3224 - acc: 0.8713\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 480us/step - loss: 0.3194 - acc: 0.8746\n"
     ]
    }
   ],
   "source": [
    "# Adjust parameters\n",
    "lr = [.001, .01, .1, .5]\n",
    "param_grid = dict(lr=lr)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(lr):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    opt = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                        epochs=20, batch_size=10, verbose=1)\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                    n_jobs=1, cv=kfold)\n",
    "\n",
    "# Fit\n",
    "grid_result3 = grid.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAErCAYAAAAi4t8iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYHFX59vHvzRJ2kpAECFsiEDZFIgZkTzQgAkFWFQR+BJAgiIKCL4tiRlEEBUFUkCAhbCJLQBZZhGhEUMABwiaENZDEAAHZg6zP+8c5Az2d7pmamZ7pTub+XFddPX3OqaqnT1fXM7UrIjAzMytikXoHYGZmCw4nDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDrJtJGiopJE3qwDhj8zhjuy+yzpE0Q9KMesdh1UlqysvPqFpPu1cnjdyppcP7kl6U9BdJ+9Q7vkaU+2lqveOw7iNpqqSF8gIuSZPyMjy0DvNeKH47i9U7gAbxw/y6OLAusCvwWUmfjojv1C8s68WuBu4E5tQ7kApG1zsAqx8nDSAimkrfSxoN3AIcKenMiJhRj7is94qIV4FX6x1HJRHxZL1jsDqKiF47AJG6oGLdv3P9l0rKxgKTgaeAt4DXgDuAfatMY2qeRh/gB8B04G1gUq7vC3wX+AswC3gHmAtcC2zWRsxTgZWAicDzwJvAP4Ctc5tlgJ8Dz+T5PVz6OSpMc2/gr8DLwP+AR4DvA0uUffaoMjSVTe8zwJXAc/kzzQTOAVbpaB8V+A7Xy/0wI4/3AvB34NAKbUcDNwH/zZ/zMeBkoG8bcS2e43oyj/MocHBJu68DD+blYRZpq3WRsmkNzdOalOP9Y47hTeB24PMV5t/S32PLymfkYen8HT+bP/cTwDGAqkyr3eW2JM5Kw9TyGCrMZwngWOABYF6ez9+BL1doW9onQ4E/AC/mPm4GxlQYpw/wLeBe0rI6L8dyDbBt0d97hWFGWbsVgJ+SfgdvkZL3lCrfU7sx0U2/ndz+06Rl+vXc37cCmwNNefqjarnOjAhvabRB+bV03+7ZpGRyG2m3wQBgR+AiSetGxAlVpjUZ2AS4kbTCeCGXrw/8JE/vT6SFbg3gi8AOknaOiJsqTK8f6Uf/OnApaSHfC7hZ0uakhWwF4HrSSm9v4DJJMyPizlYfUjoPOJC0wrsKeAXYDDgRGC1pu4h4D5hGWiGOJyWjSSWTmVoyvQOAc0krsmtJC/0w4GvAzpI2i4hnO9BHVUnaCbiCtLK6KfdFP2Aj4P+Rvq+Wtofk92/mcV4ARpFWtDtL2jIiXqkwmz+Qfsg3AO8CewITJL0LfBLYn9TPU0jf2w9IK45TKkzrY8A/gYdI39Fg4CvAjZK+GhGXtfeZs8WBPwOrkPrrPdIu1ZOBJflod2uLosvtK3ncscCQsunMaCsgSX2Am4GRpMT6G1Ji25O07A2PiOMrjDoEuJuU0C4iLbdfAa6RtG1E/LWk7STSsvwQcCFphb4KsBXwBdIKsy0/JPXTRsAv8+el5BVJQ0jL81BSwruJ9E/YGOAmSYdExLkdjKlbfjuStsjT70P67T4BDM/T/Es7fdF5tc5CC9JAlS0NYFvggzwMKSlfq8p/GlNIK5RVy+qm5nk8AAysMG7fKuWrAf8BHqkWM/BbSv6jBfbL5f8FrgOWLKnbOtddXTatsbn8KmCpsrqmXHdEhflPrdKf65D+O3qiQl98Dni/Qgxt9lEb391A0n+A7wAjK/Vhyd9DSD/E14D1ytqdlec/oUpc/wL6lZSvmef5MvB06eckJawXSVuLi5WUDy353n5eNp8Redl5GVi+wncztqz9jFx+Q+l3BqxIWvm9AixeNk6nlts2+n4G8/93flxJXIuVxdUS8xZV+mR82bS2b5lW2W/lA9JWyKIVYhpQcLmZlKc9tEr91DyfvcrK+5FW/m8BK3UmJmr42yH9U/tonuYuZe2PKOnbUUV/U4V/e7We4II0lHRsUx5+Qto0fC+X/6LgdHbP7f+vwgI435dacJpn5nHXqBDzm8ByZeWL5hVAAGtWmN7TwNNlZfflcfpVaL8oaQV4d4X5V1vwT8/1O1Wpvzr37XIlZZ3qI+CoPN4vC7T9Xm57UoW6/qRk8hatd8e1xDW6wjh/yXUHVqg7P9cNKSkbmsteKf/ecv2kXL9/SdlY2k4aa1eYzgW57hO1WG7bGG8G8yeNx0kr0PUqtD8oz2dihT6ZQeUV7jPAiyXvl8/t76DCLrgOLDctfT20Qt1Gue6KKuPukusP60xMtfztAFvm9n+r0HZRUvLplqTh3VPJ+Pza8sP+O3BeRFxc2kjSGqTdGaNJu5GWKpvOqlWmf3e1GUvakvSfweak/8r6VJhm+e6cxyLi9dKCiHhf0vPAMhHxVIVZzSbtZmmZ79KkH8mLpAP+lcJ7m7QLrajN8+tISZtUqF+RtECvA9xTVle1j6rYLL/eWKDtxvl1vk32iHhZ0n3ANqTjDfeXNWmuML3/5NfyzwCpnyFtLT5TVndv+feWTSXt5voUacXfnlcj4okK5TPza//Swi4st4VIWg5YG5gdEY9WaNLS75+qUDctIt6vUD6Tj5YnIuI1SdcBOwPTJE0m/U7vioh5XYm/RMv8+kpqqlA/KL+u3w0xdfS307JM/628YV4X3A6s1cEYCnHSACKi4hqzlKQ1SSu2/qQF48+k3SPvk/5r2p+0b72S56pMczfSls3/SGdrPUnaiviAtL99ZJVpVjur5r126kq/7/6kTdxBfJQ0u2pAfv1uO+2WrVBWsY/a0C+/zm6zVdI3v1Y7fbWlvF95RaSzmMq9l1/bqlu8Qt3zVebf8tn7VqkvV+nYS+m8F20p6OJyW1Sn+5e2P0v5dWRfISW/r/LR8Zb/SboSODoiqvVvUS3L73Z5qKZ0+a1VTB397bT0eXvLVM05aRT3HdIXe0BETCqtkLQ36cdXUeRtxgpOJO3HHBERj5RN8xxS0uguLSu8+yJi4zZbdnyafSPitY6M2EYfVdOyslmVdPZSkbhWJp1JVm5wWbvuslKV8pW7cf6dXm47oLR/K6lJ/0bEW+RdyZJWJ20djgX2JSXArbsyfT6K74iIOLOHY+rob6elfXvLVM316ivCO2jt/Dq5Ql1nV+5rA/+ukDAWIZ190W0i4g3SCvTjklbowKgfUPKfbJmWM7O6+uMtomVeOxRoe19+HVVeIakf6YyTllONu9PGeVdOuZa47qtQ11WdWW7fB5BU7XtuJe9yexJYVdKwCk0+m1/vLTK9gvOcGRGXkA6aPw5sJWlAO6NB/mxUXoa7tPwWiKmWv52WvpzvO8zfW7etP5w0ipuRX0eVFkrannRKXGenOUzSKiXTE2l30QadnGZH/IJ0DGViXnm2Iqm/pPKtkJeA1atM79ekA+unS1qnwvT6SKpVQrmAdAD7UEnbVJjXaiVvL85xfVPS2mVNTyQd0Lw4It6uUWzV9CWdkvshSSOAfUj/OV7dDfOckV9Hlc23reX2pfy6RgfmM5G0u/PnpclG0kDghJI2nSJpkKTPVKhaBliOtDvrnQKTqvrZIqKZtAtvd0kHVoljQ0krdjKmWv52/kG6pmkbSbuUNT+cbjqeAd491RFnAQcAV+QDXrOBT5DOxb6ctG+zo04nnTp7X57mu6SzIjYgnTa7cw3irioiJkr6NHAY8KSkm0kH3VcgXVOwDelsoK+XjDYF2CsfALyH9MO4LSJui4hH849tIvCwpJtIF9AtTvqRbk06HXW9GsT+oqSvko4J/VXSjaTTdpcnXT+xev4MRMQMSUeSrh24V9LlOY6RpAOQj5L2S3e324Cv5RXNHXx0ncYiwCEd3aVXUGeW2ynAl4CrJN1AOrPsmYi4qI35nEra6tsFuD+Pt3SezorAzyLi9i58jlWBOyU9Qvoveybpux5D2hVzZpWTDMpNIR03ODcfd3gDeCUifp3rv0o6cH+epG8Bd5F2ha5GWq4+QVpmXuhETDX77URESDqIdCx0sqSW6zQ2Il0ycBPpO669Wp+OtSANtHFFeJX2W5AWqJdJF9bdTrpYaBSVr+6c2t70Sfs/p5EOgL9I+m9zQ6pc0Unbp+3NoMKVuu3FQlrIryf9EN4hHUS7G/gx81/XsCLwe9IBuPerfO4NSac2tlyR/l8+uqDtcx3to3b67+Oki6pm59ifJ51RMq5C28+TDgS/zEdXUf+Myqcct9Vfk6h+2uZ83xutr35en3S1cMvVw3cA21dZLqqdclvtO662zHR0uV0UOIl0wV3LadxT24uBdGHh8fm7fqtkXntXaPthnxRZXkkH0X+QP8fs/P3Nye32pgOn4ZKO8zySpxHln4W0lXA8acX+Rv4sT5MuwB1HOkOxwzFR499Obl96Rfjr9MAV4cozNjMza5ePaZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoX1aNKQNFHSC5IeKilbQdItkh7Pr/1zuSSdKekJSQ9UeK6DmZn1sJ7e0pjE/Pd4PxaYEhHDSPebPzaX7wAMy8M44OweitHMzKro0aQREbeR7g9fahfSU9jIr7uWlF8YyZ1AP0mDMTOzummEJ/etFBFzACJiTsujFElPxZpZ0m5WLptTPgFJ40hbIyyzzDKfXm+9Lj8YzsysV7nnnntejIhB7bVrhKRRjSqUVXxiVERMACYAjBgxIpqbm7szLjOzhY6kZ4q0a4Szp55v2e2UX1/I5bNo/RD21YD/9HBsZmZWohGSxrXA/vnv/UnPT24p/798FtVmwKstu7HMzKw+enT3lKRLSQ+zHyhpFjAeOBm4XNJBwLPAl3LzG4AdgSeAecABPRmrmZnNr0eTRkTsXaVqdIW2AXyjeyMyM7OOaITdU2ZmtoBw0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwhomaUg6QtJDkh6WdGQua5I0W9K0POxY7zjNzHqzHn1GeDWSPgEcDGwKvAPcJOlPufr0iDi1bsGZmdmHGiJpAOsDd0bEPABJfwN2q29IZmZWrlF2Tz0EbCNpgKSlgR2B1XPd4ZIekDRRUv/6hWhmZg2RNCLiEeAU4BbgJuB+4D3gbGAtYDgwBzit0viSxklqltQ8d+7cngnazKwXaoikARAR50XExhGxDfBf4PGIeD4i3o+ID4BzScc8Ko07ISJGRMSIQYMG9WTYZma9SsMkDUkr5tc1gN2BSyUNLmmyG2k3lpmZ1UmjHAgHmCxpAPAu8I2IeFnSRZKGAwHMAA6pZ4BmZr1dwySNiNi6Qtl+9YjFzMwqa5jdU2Zm1vicNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMyssIZJGpKOkPSQpIclHZnLVpB0i6TH82v/esdpZtabNUTSkPQJ4GBgU2AjYIykYcCxwJSIGAZMye/NzKxOGiJpAOsDd0bEvIh4D/gbsBuwC3BBbnMBsGud4jMzM2CxegeQPQT8RNIA4C1gR6AZWCki5gBExBxJK7Y3oenTpzNq1KhWZV/+8pc57LDDmDdvHjvuuON844wdO5axY8fy4osvsueee85Xf+ihh/KVr3yFmTNnst9++81Xf9RRR7Hzzjszffp0DjnkkPnqv//977Ptttsybdo0jjzyyPnqTzrpJLbYYgv+8Y9/cPzxx89Xf8YZZzB8+HBuvfVWfvzjH89Xf84557Duuuty3XXXcdppp81Xf9FFF7H66qtz2WWXcfbZZ89Xf+WVVzJw4EAmTZrEpEmT5qu/4YYbWHrppTnrrLO4/PLL56ufOnUqAKeeeirXX399q7qlllqKG2+8EYATTzyRKVOmtKofMGAAkydPBuC4447jn//8Z6v61VZbjYsvvhiAI488kmnTprWqX2eddZgwYQIA48aN47HHHmtVP3z4cM444wwA9t13X2bNmtWqfvPNN+enP/0pAHvssQcvvfRSq/rRo0dzwgknALDDDjvw1ltvtaofM2YMRx99NMB8yx142fOyt+Aue9U0RNKIiEcknQLcArwB3A+8V3R8SeOAcQBLLLFEt8RoZmagiKh3DPORdBIwCzgCGJW3MgYDUyNi3bbGHTFiRDQ3N/dEmGZmCw1J90TEiPbaNcoxDVp2PUlaA9gduBS4Ftg/N9kfuKY+0ZmZGTTI7qlscj6m8S7wjYh4WdLJwOWSDgKeBb5U1wjNzHq5hkkaEbF1hbKXgNF1CMfMzCpomN1TZmbW+Jw0zMysMCcNMzMrzEnDzMwKK5Q0JG3X3YGYmVnjK7qlcbOkJyR9V9Kgbo3IzMwaVtGk8TngX8CJwExJv5c0svvCWjg1NTUhqdNDU1NTvT+CmfVyHbqNSL747kDSbczXBqYDvwUujIiXuyXCDlpQbyPScsOxlhuwmZn1pG65jUhEvBQRP4+IdYDtgBeBXwCzJU2StGHnwjUzswVBp86ekrQj8C1gM+AF4EJgJHCvpENrF56ZmTWSwklD0sqSvifpaeB6oB+wL7B6RHydtLvqHOAH3RKpmZnVXaF7T0maDIwB/gdcDJwVEQ+XtomI9yX9Hjis5lGamVlDKHrDwmHAkcBFEfFGG+0eBD7b5ajMzKwhFdo9FRGfjIiz20kYRMTrEfG32oRm1ppPWTarv6JXhI+RdHiVum/kA+Nm3aqpqYmIqDiMHDmSkSNHVq2PCCcNsxooeiD8BGCZKnVL5XozM1vIFU0a6wH3VqmbBqxfm3DMzKyRFU0aiwDLVqlbDli8NuGYmVkjK5o07gf2qVK3D/BAbcIxM7NGVjRpnAbsLukKSZ+XtIGk7SRdAewG/LyrgUj6tqSHJT0k6VJJS+ZbkzwtaVoehnd1PmZm1nmFrtOIiKslHQH8BNg9Fwt4A/hWRFzVlSAkrUq6LckGEfGWpMuBvXL1dyPiyq5M38zMaqPoxX1ExK8kTQK2AAaQblb4j/au3ehgLEtJehdYGvhPjaZrZmY10tG73L4eETdHxO8j4s+1ShgRMRs4FXgWmAO8GhF/ztU/kfSApNMlLVFpfEnjJDVLap47d24tQjIzswo6lDQk9Ze0qaRtyoeuBCGpP7AL8DFgFWAZSfsCx5FO990EWAE4ptL4ETEhIkZExIhBg/xgQTOz7lL0hoVLAhOBL5OOZVSyaBfi2BZ4OiLm5vldBWwRERfn+rclnQ8c3YV5mJlZFxU9pnECMArYH7gI+AbpjrdjgcHAEV2M41lgM0lLA28Bo4FmSYMjYo4kAbsCD3VxPm0aeuyfunPybXruqZfqHsOMk3eq27zNbMFQdPfUHsCPgD/k93dFxPkRMZJ0DccXuhJERNwFXEm66vzBHNcE4BJJD+aygcCPuzIfMzPrmqJbGmsAD+dnZrxL6/tQTQTOp4tbGxExHhhfVvy5rkzTzMxqq+iWxkt8dBuRmcBGJXUDSTctNDOzhVzRLY07gU8BNwKTgRMlLQe8BxwF3N494ZmZWSMpmjROIe2ignRcYW3SMY5FSQnl0NqHZmZmjabobUSageb89+vAHvlCuyUi4rVujM/MzBpIu8c0JPWRdK+kz5eWR8TbThhmZr1Lu0kjIt4hXan9XveHY2Zmjazo2VO3AJ9vt5WZmS3Uih4I/xVwsaTFgD+SbioYpQ0i4qkax2ZmZg2maNL4W379DvDtKm26cu8pMzNbABRNGgd0axRmZrZAKHrK7QXdHYiZmTW+Dj1Pw8zMereiz9OY2E6TiIiDahCPmZk1sKLHND5H2dlSpCfpLQe8kgfr5fw8Ej+PxBZ+RY9pDK1Unh/z+ltgnxrGZGZmDarolkZFEXGbpNNJ13FsVZuQFl6v3H4Jr95xaZttnjllTNW6vlvuTb+tnJ/NrH66lDSyp0i3Tbd29NtqH6/0zWyB1qWzp/IV4mOBWTWJxszMGlqhpCHpLxWG24H/AF8FTu1qIJK+LelhSQ9JulTSkpI+JukuSY9LukxSn67Ox6w3a2pqQlKnh6ampnp/BKuzolsaiwAqG14HrgJGR8S5XQlC0qrAt4AREfEJ0i1J9iI9/On0iBgGvAz4tF6zLmhqaiIiKg4jR45k5MiRVesjwknDCp89Naqb44AUy1KS3gWWJt0U8XOkLRmAC4Am4OweiMXMzCpp67+KnhyAI4A3gLnAJcBA4ImS+tWBh6qMO470ZMHmvn37BumakgCiubk5mpubW5WNHz8+IiIGDx78YdnGG28cQ465PpbdaPtWbVc97IIYtMcJrcpW2P7wGHLM9a3KllprkxhyzPWx1FqbtCofcsz1scL2h7cqG7THCbHqYRe0Klt2o+1jyDHXR5+V1vqwbNFlV4ghx1wffbfcu1Xblfc/I1be/4xWZX233DuGHHN9LLrsCh+W9VlprQ59pkid+eEwZsyYiIgYM2ZMq/KIiHPOOadV2bXXXlvXz9RnlXVjsYFr1PV7Gj9+fKeXvYiIgw8+uFXb2bNnx7XXXtuq7Jxzzuny9zR79uxWZQcffHBERCy77LIflg0ePDgiYoH/TBtvvLE/U/HP1FxkXa280m1TPq12YETsV6HuIuD5iDi63QlVn35/YDLwFdKFglfk9+MjYu3cZnXghojYsK1pjRgxIpqbmzsVRz0vDGsEXb04ra4X9/3+WABW/urJdYthQb+4b9SoUQBMnTq1rnFYfUi6JyJGtNeu6DGNLwJ/rlJ3M7Br0cCq2BZ4OiLmRsS7pGMlWwD98hlaAKuRDrybmVmdFE0aqwIzq9TNyvVd8SywmaSlJQkYDfwb+CuwZ26zP3BNF+djZmZdUDRpvAysXaVubdKZVJ0WEXcBVwL3Ag/muCYAxwDfkfQEMAA4ryvzMTOzril6RfitwPckXRcRz7cUSloJOJ70DPEuiYjxwPiy4qeATbs6bTMzq42iSeME4F/A45Ku56NdUmOAt4Hvd094ZmbWSIpepzFD0ibAj4DtSLuKXgSuJp3h9Ez3hWhmZo2i8A0LI2IG8H/dF4qZmTW6oveeGiRpnSp160gaWNuwzMysERU9e+os4Kgqdd/O9WZmtpArmjS2Il3EV8mfgS1rE46ZmTWyokmjP/BqlbrXSAfGzcxsIVc0acwCPlOl7jOkO9KamdlCrmjSuBI4XlKrO7Ll98cCl9c6MDMzazxFT7n9EbANcK2k54DZpIv7VgbuBH7YPeGZ9R71vsvyc0+9VPc4FvQ7BfcGRS/umydpJLAfH13c9wTpIPjFEfFe94VoZmaNoiMX970LTMxDK5KWjoh5tQzMzMwaT9FjGhVJGiXpfOC5GsVjZmYNrPCWRgtJw0i3E9mP9AjWt0kPTTLrVq/cfgmv3nFpm22eOWVM1bq+W+5Nv632qXVYZr1KoaQhqS+wF+lBSJ8BRHrG7CnAKRFR7RoOs5rpt9U+Xumb1VnV3VOSFpG0k6TLSddhnE3asvgZ6QpwATc5YZiZ9R5tbWnMBlYE5gGTgQuBWyMi8paHmZn1Mm0ljZXy693AH4GpERHdH5KZmTWqts6e2hr4HbAx6Yrv5ySdJWmzWgchaV1J00qG1yQdKalJ0uyS8h1rPW8zMyuuatKIiDsiYhwwGNiX9LjXg4E7gGmkA+H9axFEREyPiOERMRz4NGmX2NW5+vSWuoi4oRbzMzOzzmn3Oo2I+F9EXBoRXwDWAI4D3iAdCJ8s6VZJX61hTKOBJ/0IWTOzxtOhi/siYk5E/CwiNgQ2IZ1R9UngohrGtBdQejL+4ZIekDRRUsUtG0njJDVLap47d24NQzEzs1KdviI8Iu6JiG8CqwB71CIYSX2ALwJX5KKzgbWA4aTTfk+rEsuEiBgRESMGDRpUi1DMzKyCLt1GBCAi3ouIP9YiGGAH4N6IeD5P+/mIeD8iPgDOBTat0XzMzKwTupw0amxvSnZNSRpcUrcb8FCPR2RmZh/q8L2nuoukpUm3XT+kpPhnkoaTztSaUVZnZmY9rGGSRr61+oCysv3qFI6ZmVXQaLunzMysgXVqSyOf+roZ6VqNOyPivzWNysy6hW8vb13VmedpjCRdrf0BsATwnqQ9I2JKrYMzs9ry7eWtqzqze+p04DsRMZB0G5FLgTNqGpWZmTWktp6n8StJy1WoGgr8AdI1GqSn9g3plujMzKyhtLWlsSbwmKS9y8rvAk6XtIGkTYHjc5mZmS3k2rrL7U7AYcBPJU2RtE6u+jrpflMPAXcCS+PrJ8zMeoU2j2lExNXA+qTbojdL+jHwfERsCSwP9I2IzSLiqe4P1czM6q3IrdHfiohjSfd92gz4t6QxEfFGRLze7RGamVnDaDNpSFokP1VvI2BGRGwLfB84R9IfJa3eI1GamVlDaOvsqU8CjwKPAPcBsyTtFhG/B9YDngEelHSMpIa5HYmZmXWftrY0JpCSxWCgL/Br4EJJS0bE6xFxBDAS2Bm4v9sjNTOzumsraWwATMjPtHiddAHfMqRHvgIQEfdHxFbAqd0bppmZNYK2div9CzhW0ivA/4DDgZeA+c6Uiojzuyc8MzNrJG1taRxEurfUv4AHgc8Be+arwM3MrBequqURETOAbfLDkfpExCs9FpWZmTWkds96yg9HmtcDsZiZWYPzQ5jMzKywhkga+QLCaSXDa5KOlLSCpFskPZ5f+9c7VjOz3qwhkkZETI+I4RExHPg0aXfY1cCxwJSIGAZMye/NzKxOGiJplBkNPBkRzwC7ABfk8guAXesWlZmZNWTS2Iv0NECAlSJiDkB+XbHSCJLGSWqW1Dx37tweCtPMrPdpqKQhqQ/wReCKjowXERMiYkREjBg0aFD3BGdmZo2VNIAdgHsj4vn8/nlJgwHy6wt1i8zMzBouaezNR7umAK4F9s9/7w9c0+MRmZnZhxomaeQrz7cDriopPhnYTtLjue7kesRmZmZJwzwHI195PqCs7CXS2VRmZtYAGmZLw8zMGp+ThpmZFeakYWZWUFNTE5I6PTQZadwZAAAK50lEQVQ1NdX7I3RZwxzTMDNrdE1NTVVX/KNGjQJg6tSpPRZPPXhLw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwhomaUjqJ+lKSY9KekTS5pKaJM2WNC0PO9Y7TjOz3qyRnqfxS+CmiNhTUh9gaWB74PSIOLW+oZmZGTRI0pC0PLANMBYgIt4B3pFUz7DMzKxMo+yeWhOYC5wv6T5Jv5O0TK47XNIDkiZK6l/HGM3Mer1GSRqLARsDZ0fEp4A3gWOBs4G1gOHAHOC0SiNLGiepWVLz3LlzeyhkM7Pep1GSxixgVkTcld9fCWwcEc9HxPsR8QFwLrBppZEjYkJEjIiIEYMGDeqhkM3Mep+GSBoR8RwwU9K6uWg08G9Jg0ua7QY81OPBmZnZhxriQHj2TeCSfObUU8ABwJmShgMBzAAOqV94ZmbWMEkjIqYBI8qK96tHLGZmVllD7J4yM7MFg5OGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU1TNKQ1E/SlZIelfSIpM0lrSDpFkmP59f+9Y7TzKw3a5ikAfwSuCki1gM2Ah4BjgWmRMQwYEp+b2ZmddIQSUPS8sA2wHkAEfFORLwC7AJckJtdAOxanwjNzAxAEVHvGJA0HJgA/Ju0lXEPcAQwOyL6lbR7OSLm20UlaRwwLr9dF5je7UF3j4HAi/UOYgHm/us692HXLMj9NyQiBrXXqFGSxgjgTmDLiLhL0i+B14BvFkkaCwtJzRExot5xLKjcf13nPuya3tB/DbF7CpgFzIqIu/L7K4GNgeclDQbIry/UKT4zM6NBkkZEPAfMlLRuLhpN2lV1LbB/LtsfuKYO4ZmZWbZYvQMo8U3gEkl9gKeAA0hJ7XJJBwHPAl+qY3w9YUK9A1jAuf+6zn3YNQt9/zXEMQ0zM1swNMTuKTMzWzA4aZiZWWFOGt1I0hckTZf0hKT5rmaXtISky3L9XZKGltQdl8unS9q+pHyipBckPdQzn6IxdLYvJQ2Q9FdJb0j6dU/H3YgK9OU2ku6V9J6kPesRYyMr0H9jJc2VNC0PX6tHnN3FSaObSFoU+A2wA7ABsLekDcqaHQS8HBFrA6cDp+RxNwD2Aj4OfAE4K08PYFIu6zW60pfA/4ATgKN7KNyGVrAvnwXGAr/v2egaX8H+A7gsIobn4Xc9GmQ3c9LoPpsCT0TEUxHxDvAH0m1RSpXeJuVKYLQk5fI/RMTbEfE08ESeHhFxG/DfnvgADaTTfRkRb0bE7aTkYQX6MiJmRMQDwAf1CLDBFVkWF2pOGt1nVWBmyftZuaxim4h4D3gVGFBw3N6kK31prXnZ6pqi/beHpAfynbtX75nQeoaTRvdRhbLy85urtSkybm/Slb601txPXVOk/64DhkbEJ4Fb+WgLeKHgpNF9ZgGl/2GsBvynWhtJiwF9Sbueiozbm3SlL601L1td027/RcRLEfF2fnsu8Okeiq1HOGl0n38BwyR9LF/lvhfptiilSm+Tsifwl0hXW14L7JXPCPoYMAy4u4fibkRd6UtrrUhfWnXt9l/L/fKyL5KeDbTwiAgP3TQAOwKPAU8C38tlPwK+mP9eEriCdKD7bmDNknG/l8ebDuxQUn4pMAd4l/Rfz0H1/pwLQF/OIG11vJH7bIN6f54G78tNcj+9CbwEPFzvmBtpKNB/PwUeBu4H/gqsV++Yazn4NiJmZlaYd0+ZmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGtZQ8h1CQ9La9Y6lIyTNkDSpDvOdlPurZZiX71B7YBemeaSk3WsZpy08Gulxr2YLst2A1+o077mki8gAVgKOAM6T9FpEXNmJ6R0J3A5cVaP4bCHipGFWJt9pePFIdzEtJCLu68aQ2vNORNzZ8kbSFNJN9b5GuuOvWc1495QtkCSNlDRF0uuS3pR0s6RPlLX5vKQbJM3Ju20eknRUybNJWtrNkHSxpAMlPQq8A+wkaWje5XOIpB/l6bwi6TpJq1WYxqSS9y272TaTdImk1yT9R9KZkpYsG3fNHOe8/ICt0ySNy+MP7WjfRMQbpCuW1yibzyb5rquzJL2VHyR0kqSlSj8HMATYp2SXV+nn2kjStZJeztO4Q9LWHY3RFlze0rAFjqSdgGuAPwH75uJjgL9L+mREtNy6ek1gCvAr0vM0RgBNwCCg/IlrnwWGAz8EXiDdeqTFccA/gAOBFYHTgEuAkQXCvYh065fdgc3z/F8GxufP0ge4hXQblMPyvL9Gun9Wp+SkuDpwT1nVGsA00oO8Xic95OsHpH7aK7fZDbiBdAuMplw2N093Y+DvwH3AwcA84OvArZK2iIjy+dnCqN73MfHgoXQgPTEugLXbaPMEMKWsbHngReCMKuOI9E/S90gr7UVK6maQVoArl40zNMfyt7Lyo3P5KmXTmFThc/ywbNzrgcdK3o/L7TYti/X+XD60nf6aRLpP1GJ5WAX4Nem+UZ9pY7yW/tiX9LClAWWf5eIK40wh3XyvT0nZornsj/Vedjz0zOAtDVugSBoGrAWclG+B3mIe8E9gm5K2g0n/LX+BtDItbb8i8FzJ+zsjovR9qT+VvX8wv65B+7cVrzTutiXvNwOejYgP72IcESFpMvDJdqbdYlXSDSw/nASwd0TcVdpI0vKkpLknaUtk8ZLqYaSbE1aUd2GNBE4CPijr+1uBfQrGags4Jw1b0KyYX8/LQ7lnASQtQrpl9SqkxPEo8BawK2nFuWTZeHPamGf5czlanpVQPo2i4y5R8n4waZdUuecLTLvFC8BOpGOUawE/BiZKuj8iHi1pdz4pYf2AtJvqTdLjS39D+59lBdJWxQl5mI+kRSLCj4hdyDlp2IKm5b/h40j/4ZZrOeNpLdIxjP0i4uKWSkk7V5luvW73PAfYoEL5Sh2YxrsR0Zz/vlvSvcADpGMvOwHkg++7AE0R8cuWESVtWHAer5B2Y/0GuLBSAyeM3sFJwxY000n73D8eESe30W7p/PrhbhtJi9N4u1HuBA6QtGnLLqp8yu8enZ1gREyX9Bvg25I2iYh/kbZuFqX1bixIx17KvQ0sVVoQEW9K+juwEXCvE0Tv5aRhjeoLksqPMbwaEbdI+gZwTT7z6HLSAfCVgC1Ixwd+QTo4+wzwE0nvk1aW3+658AubRDrz6ypJ3yOdqfQ1oH+u7+zK+WTSQfYfADtHxKuS7gSOkjSH1GcHko6HlPs3sLWkMaTjPi9GxAzgO8BtwM2SziNtJQ0ENgYWjYjyM9JsIeTrNKxR/Yr0JL7S4XSAiLiBdMB7GeB3wM3Az4CVSQfDiXRh3q6kld6FpN0qt5FWpg0jx/l50u6k3wIXkC7M+01u8monp/sCcCYwRtKncvHepNNwf0NKVs+Rrh4vdxxpi+5y0uNNm/I07yU91e+lPO0/A78ENiT1rfUCfnKfWQOSdD2wfkSsVe9YzEp595RZnUn6Dun55Y8DywFfIh3APrSecZlV4qRhVn9vk463rEE6WD0d+FpEVDql2KyuvHvKzMwK84FwMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvs/wOe6iWsN+bYcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.49%  Stdev: 3.45% Params: {'lr': 0.001}\n",
      "Accuracy: 81.52%  Stdev: 5.08% Params: {'lr': 0.01}\n",
      "Accuracy: 77.23%  Stdev: 3.39% Params: {'lr': 0.1}\n",
      "Accuracy: 58.42%  Stdev: 10.23% Params: {'lr': 0.5}\n",
      "\n",
      "Best: 84.49% using {'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid_result3\n",
    "\n",
    "means = [x*100 for x in grid_result.cv_results_['mean_test_score']]\n",
    "stds = [x*100 for x in grid_result.cv_results_['std_test_score']]\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "# Plot the accuracy of all the things tried in this gridsearch\n",
    "var = [str(x['lr']) for x in params]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Learning Rate', fontsize=16)\n",
    "ax.set_ylabel('% Accuracy', fontsize=16)\n",
    "ax.set_ylim(60,100)\n",
    "ax.axhline(80, color='k', linestyle='--', linewidth=1)\n",
    "ax.axhline(90, color='k', linestyle='--')\n",
    "ax = plt.bar(var, means, yerr=stds, capsize=8)\n",
    "plt.title('Parameter combinations tested', fontsize=20, y=1.05)\n",
    "plt.show()\n",
    "\n",
    "# Report Results\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Accuracy: {mean:.2f}%  Stdev: {stdev:.2f}% Params: {param}')\n",
    "print()\n",
    "print(f\"Best: {grid_result.best_score_*100:.2f}% using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stick with the default learning rate, is what I say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
